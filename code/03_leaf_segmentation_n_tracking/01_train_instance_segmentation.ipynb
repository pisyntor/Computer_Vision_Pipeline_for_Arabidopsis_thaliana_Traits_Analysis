{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dd40526b",
   "metadata": {},
   "source": [
    "## Fine-tune YOLO 11 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4bc8156-5271-4d5e-a10f-b27dfaad304b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "model = YOLO(\"yolo11m-seg.pt\") # start from the pretrained model\n",
    "model.train(\n",
    "    # paths\n",
    "    project=\".\",\n",
    "    name=\"yolo_training_run\",\n",
    "    data=\"../data_meta/yolo_train_ds.yaml\",\n",
    "\n",
    "    # training params\n",
    "    epochs=300,\n",
    "    imgsz=640,\n",
    "    device=\"cuda:1\",\n",
    "\n",
    "    # augmentation params\n",
    "    degrees=360,\n",
    "    copy_paste=0.4,\n",
    "    copy_paste_mode=\"mixup\",\n",
    "    shear=10,\n",
    "    perspective=0.0003,\n",
    "    flipud=0.5,\n",
    "    hsv_h=0.3,\n",
    "    bgr=0.1,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47817fff-5f92-46f2-87e9-3de49c26e40e",
   "metadata": {},
   "source": [
    "### Sanity check\n",
    "Check some image as the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9462046-4a10-44e5-94ea-050b457a8fc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = model(\n",
    "    \"../data/yolo_train/images/val/TOU-J-3/rep_06/1000269_2022_05_23_12_01_21-6-23-TB06-RGB1_pot_D2_TOU-J-3-06.png\", retina_masks=True\n",
    ")\n",
    "results[0].show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7d2d10e-9cb7-4663-94f2-d73042ae40b4",
   "metadata": {},
   "source": [
    "## Yolo v8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7df6a241-fd2b-4ca2-86aa-0acea4bc97a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "model = YOLO(\"yolov8m-seg.pt\")\n",
    "model.train(\n",
    "    project=\"./runs/segment/yolov8\",\n",
    "    name=\"yolo_v8_training_run\",\n",
    "    data=\"../data_meta/yolo_train_ds.yaml\",\n",
    "    epochs=300,\n",
    "    imgsz=640,\n",
    "    device=\"cuda:1\",\n",
    "    patience=20,\n",
    "    degrees=360,\n",
    "    copy_paste=0.4,\n",
    "    copy_paste_mode=\"mixup\",\n",
    "    shear=10,\n",
    "    perspective=0.0003,\n",
    "    flipud=0.5,\n",
    "    hsv_h=0.3,\n",
    "    bgr=0.1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9a59028-b406-4763-b0b6-4c7cc899df86",
   "metadata": {},
   "source": [
    "## Sanity check\n",
    "- Check some image as the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b2ed0a2-5e35-4fd6-b07c-455f730f5063",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = model(\n",
    "    \"../data/yolo_train/images/val/TOU-J-3/rep_06/1000269_2022_05_23_12_01_21-6-23-TB06-RGB1_pot_D2_TOU-J-3-06.png\", retina_masks=True\n",
    ")\n",
    "results[0].show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ab45726-1dbc-4992-b857-ad2dd1b29224",
   "metadata": {},
   "source": [
    "# Fine-tune SAM2 model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ad5ff2d-9907-42ad-8ecc-2fc5dd3a675a",
   "metadata": {},
   "source": [
    "### Set data parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f059d33-459b-4587-9051-89f963c0d4b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "os.environ[\"HYDRA_FULL_ERROR\"] = \"1\"\n",
    "sys.path.insert(0, \"../src\")\n",
    "SAM_PATH = \"../thirdparty/segment-anything-2/\"\n",
    "sys.path.insert(1, SAM_PATH)\n",
    "from omegaconf import OmegaConf\n",
    "from hydra.utils import instantiate\n",
    "from hydra import compose, initialize, initialize_config_module\n",
    "from PIL import Image as PILImage\n",
    "from saveload import read_image, read_masks, mask_joined_to_masks_dict, _imread_func\n",
    "from masks import OUT_OF_LIST_COLOR, DEFAULT_COLORS\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from dataset import load_dataset\n",
    "from training.dataset.vos_raw_dataset import VOSRawDataset, VOSVideo, VOSFrame\n",
    "OmegaConf.register_new_resolver(\"times\", lambda a, b: a * b)\n",
    "OmegaConf.register_new_resolver(\"divide\", lambda a, b: a // b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d8aab1e-4a46-4cf1-ae3f-a9fb80764687",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGES_ROOT = (\n",
    "    \"\"  # root folder with images\n",
    ")\n",
    "MASKS_ROOT = (\n",
    "    \"\"  # root folder with labels\n",
    ")\n",
    "ds = load_dataset(images_root=IMAGES_ROOT, masks_root=MASKS_ROOT)\n",
    "ds = ds[ds[\"nn_role\"] == \"train\"]\n",
    "print(f\"Prepared ds with {len(ds)} images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d5943f6-0e71-4acc-8c9b-f66bdac95892",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask_joined_to_masks_dict_no_error(mask: np.ndarray) -> dict[int, dict]:\n",
    "    \"\"\"Split joined masks to separate masks.\n",
    "\n",
    "    Args:\n",
    "        mask (np.ndarray): joined masks\n",
    "\n",
    "    Returns:\n",
    "        list: masks in SAM2 format\n",
    "    \"\"\"\n",
    "    masks = {}\n",
    "\n",
    "    all_masks_colors = set(\n",
    "        tuple(x.tolist()) for x in np.unique(mask.reshape(-1, 3), axis=0)\n",
    "    )\n",
    "    strange_colors = []\n",
    "    for c in all_masks_colors:\n",
    "        if not (c in DEFAULT_COLORS or c == OUT_OF_LIST_COLOR or c == (0, 0, 0)):\n",
    "            print(f\"Problem with parsing color {c}\")\n",
    "            strange_colors.append(c)\n",
    "        # assert (\n",
    "        #     c in DEFAULT_COLORS or c == OUT_OF_LIST_COLOR or c == (0, 0, 0)\n",
    "        # ), f\"Unknown color {c}\"\n",
    "\n",
    "    for i, color in enumerate(DEFAULT_COLORS + [OUT_OF_LIST_COLOR] + strange_colors):\n",
    "        if color not in all_masks_colors:\n",
    "            pass\n",
    "\n",
    "        mask_i = np.all(mask == color, axis=-1)\n",
    "        if mask_i.sum() > 0:\n",
    "            masks[i] = {\"segmentation\": mask_i, \"_detection_index\": i}\n",
    "    return masks\n",
    "\n",
    "\n",
    "class LeafPalettisedPNGSegmentLoader:\n",
    "    def __init__(self, video_ds_part: pd.DataFrame):\n",
    "        \"\"\"\n",
    "        SegmentLoader for datasets with masks stored as palettised PNGs.\n",
    "        video_png_root: the folder contains all the masks stored in png\n",
    "        \"\"\"\n",
    "        self.video_ds_part2 = video_ds_part\n",
    "\n",
    "        self.frame_id_to_png_filename = {}\n",
    "        for _, row in self.video_ds_part2.iterrows():\n",
    "            self.frame_id_to_png_filename[row[\"image_num\"]] = row[\"mask_path\"]\n",
    "\n",
    "    def load(self, frame_id):\n",
    "        \"\"\"\n",
    "        load the single palettised mask from the disk (path: f'{self.video_png_root}/{frame_id:05d}.png')\n",
    "        Args:\n",
    "            frame_id: int, define the mask path\n",
    "        Return:\n",
    "            binary_segments: dict\n",
    "        \"\"\"\n",
    "        mask_path = self.frame_id_to_png_filename[frame_id]\n",
    "        masks = mask_joined_to_masks_dict_no_error(_imread_func(mask_path))\n",
    "\n",
    "        binary_segments = {}\n",
    "        for i, m in masks.items():\n",
    "            # binary_segments[m['detection_index']] = torch.from_numpy(m['segmentation'])\n",
    "            binary_segments[i] = torch.from_numpy(m[\"segmentation\"])\n",
    "\n",
    "        return binary_segments\n",
    "\n",
    "    def __len__(self):\n",
    "        raise NotImplementedError()\n",
    "        return\n",
    "\n",
    "\n",
    "class LeafPNGRawDataset(VOSRawDataset):\n",
    "    leaf_ds = ds\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        img_folder,\n",
    "        gt_folder,\n",
    "        file_list_txt=None,\n",
    "        excluded_videos_list_txt=None,\n",
    "        sample_rate=1,\n",
    "        is_palette=True,\n",
    "        single_object_mode=False,\n",
    "        truncate_video=-1,\n",
    "        frames_sampling_mult=False,\n",
    "    ):\n",
    "        self.img_folder = img_folder\n",
    "        self.gt_folder = gt_folder\n",
    "        self.sample_rate = sample_rate\n",
    "        self.is_palette = is_palette\n",
    "        self.single_object_mode = single_object_mode\n",
    "        self.truncate_video = truncate_video\n",
    "\n",
    "        assert self.img_folder == None, f\"img_folder {self.img_folder} is not None\"\n",
    "        assert self.gt_folder == None, f\"gt_folder {self.gt_folder} is not None\"\n",
    "        assert file_list_txt == None, f\"file_list_txt {file_list_txt} is not None\"\n",
    "        assert (\n",
    "            frames_sampling_mult == False\n",
    "        ), f\"frames_sampling_mult {frames_sampling_mult} is not False\"\n",
    "        assert (\n",
    "            self.single_object_mode == False\n",
    "        ), f\"single_object_mode {self.single_object_mode} is not False\"\n",
    "        assert self.is_palette == True, f\"is_palette {self.is_palette} is not True\"\n",
    "        assert (\n",
    "            self.truncate_video == -1\n",
    "        ), f\"truncate_video {self.truncate_video} is not -1\"\n",
    "\n",
    "        # Read the subset defined in file_list_txt\n",
    "        self.video_names2 = sorted(\n",
    "            set(f\"{row['plant']}/{row['rep']}\" for (_, row) in self.leaf_ds.iterrows())\n",
    "        )\n",
    "\n",
    "    def get_video(self, idx):\n",
    "        \"\"\"\n",
    "        Given a VOSVideo object, return the mask tensors.\n",
    "        \"\"\"\n",
    "        # print(\"called\")\n",
    "        video_name = self.video_names2[idx]\n",
    "        plant, rep = video_name.split(\"/\")\n",
    "        video_part = self.leaf_ds[\n",
    "            (self.leaf_ds[\"plant\"] == plant) & (self.leaf_ds[\"rep\"] == rep)\n",
    "        ]\n",
    "        # if self.is_palette:\n",
    "        segment_loader = LeafPalettisedPNGSegmentLoader(video_ds_part=video_part)\n",
    "\n",
    "        frames = []\n",
    "        for _, row in video_part.iterrows():\n",
    "            fid = row[\"image_num\"]\n",
    "            frames.append(VOSFrame(fid, image_path=row[\"image_path\"]))\n",
    "        video = VOSVideo(video_name, idx, frames)\n",
    "        return video, segment_loader\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.video_names2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edcce335-f85f-4d9d-a9ac-c5e4c3fdb309",
   "metadata": {},
   "source": [
    "### Build trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44340289-ac17-4e68-983b-f6631ba9cb0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with initialize(config_path=os.path.join(SAM_PATH, \"sam2\"), version_base=\"1.2\"):\n",
    "    # Compose the configuration\n",
    "    cfg = compose(\n",
    "        config_name=\"configs/sam2.1_training/sam2.1_hiera_b+_MOSE_finetune.yaml\"\n",
    "    )\n",
    "    cfg.launcher.experiment_log_dir = \"sam2_training_run\"\n",
    "    video_ds = cfg.trainer.data.train.datasets[0].dataset.datasets[0].video_dataset\n",
    "    video_ds._target_ = \"__main__.LeafPNGRawDataset\"\n",
    "    video_ds.img_folder = None\n",
    "    video_ds.gt_folder = None\n",
    "    video_ds.file_list_txt = None\n",
    "    cfg.scratch.num_train_workers = 5\n",
    "    cfg.trainer.checkpoint.model_weight_initializer.state_dict.checkpoint_path = (\n",
    "        SAM_PATH + \"/checkpoints/sam2.1_hiera_base_plus.pt\"\n",
    "    )\n",
    "    cfg.trainer.max_epochs = 20\n",
    "    # cfg.trainer.accelerator = \"cuda:1\" # is set in local_rank\n",
    "    # cfg.trainer.max_epochs = cfg.scratch.num_epochs\n",
    "\n",
    "\"\"\"Single GPU process\"\"\"\n",
    "os.environ[\"MASTER_ADDR\"] = \"localhost\"\n",
    "os.environ[\"MASTER_PORT\"] = str(8085)\n",
    "os.environ[\"RANK\"] = str(0)  # hm, what does this parameter do?\n",
    "os.environ[\"LOCAL_RANK\"] = str(1)\n",
    "os.environ[\"WORLD_SIZE\"] = str(1)\n",
    "trainer = instantiate(cfg.trainer, _recursive_=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65af9ba2-7c89-4c40-89f8-7d1011bccd30",
   "metadata": {},
   "source": [
    "### Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aacf198c-37c4-499b-b9b6-aa4f09e9ad26",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc9ea1f1-b331-4a37-955b-f4df17c8cd18",
   "metadata": {},
   "source": [
    "### Sanity check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67e0baeb-f3fb-466b-9f2a-9fdde9b2cdcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sam2.build_sam import build_sam2\n",
    "\n",
    "sam2 = build_sam2(\n",
    "    \"configs/sam2.1/sam2.1_hiera_b+.yaml\",\n",
    "    \"sam2_training_run/checkpoints/checkpoint.pt\",\n",
    "    device=\"cuda:1\",\n",
    "    apply_postprocessing=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3085973b-e3d7-4556-a074-6c08b7fc6b05",
   "metadata": {},
   "source": [
    "# Detectron 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b49d691b-cc5d-4259-a91b-5870b00f143b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m pip install 'git+https://github.com/facebookresearch/detectron2.git'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91ee0ff5-5c7b-4c1f-ad25-4ed384b1e5c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch, detectron2\n",
    "from datetime import datetime\n",
    "from detectron2.data.datasets import register_coco_instances\n",
    "from detectron2.data import DatasetCatalog, MetadataCatalog\n",
    "from detectron2.utils.visualizer import Visualizer\n",
    "from detectron2.utils.visualizer import ColorMode\n",
    "from detectron2 import model_zoo\n",
    "from detectron2.config import get_cfg\n",
    "from detectron2.engine import DefaultPredictor\n",
    "from detectron2.engine import DefaultTrainer\n",
    "import matplotlib.pyplot as plt\n",
    "from detectron2.data import MetadataCatalog, DatasetCatalog\n",
    "from detectron2.data.datasets import register_coco_instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85bd81f3-5471-497d-9603-823e39038a4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = get_cfg()\n",
    "cfg.merge_from_file(model_zoo.get_config_file(\"COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml\"))\n",
    "cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.5\n",
    "cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(\"COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml\")\n",
    "predictor = DefaultPredictor(cfg)\n",
    "outputs = predictor(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d14ae9a3-15ac-42bb-b3fc-4370658282c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the visualizer\n",
    "visualizer = Visualizer(image[:, :, ::-1], MetadataCatalog.get(cfg.DATASETS.TRAIN[0]), scale=1.2)\n",
    "out = visualizer.draw_instance_predictions(outputs[\"instances\"].to(\"cpu\"))\n",
    "# Convert the output to a NumPy array for matplotlib\n",
    "output_image = out.get_image()[:, :, ::-1]\n",
    "# Display the image using matplotlib\n",
    "plt.figure(figsize=(12, 8))  # Adjust the figure size as needed\n",
    "plt.imshow(output_image)\n",
    "plt.axis(\"off\")  # Hide axis\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78c974c1-71af-450f-a9f8-cc754de75587",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define dataset name\n",
    "DATA_SET_NAME = \"leaf\"  # Replace with your dataset name\n",
    "\n",
    "# TRAIN SET - Manually specify the paths\n",
    "TRAIN_DATA_SET_NAME = f\"{DATA_SET_NAME}-train\"\n",
    "TRAIN_DATA_SET_IMAGES_DIR_PATH = \"\"  # Replace with the path to your training images directory\n",
    "TRAIN_DATA_SET_ANN_FILE_PATH   = \"\"  # Replace with your training JSON file path\n",
    "\n",
    "register_coco_instances(\n",
    "    name=TRAIN_DATA_SET_NAME, \n",
    "    metadata={}, \n",
    "    json_file=TRAIN_DATA_SET_ANN_FILE_PATH, \n",
    "    image_root=TRAIN_DATA_SET_IMAGES_DIR_PATH\n",
    ")\n",
    "\n",
    "# TEST SET - Manually specify the paths\n",
    "TEST_DATA_SET_NAME = f\"{DATA_SET_NAME}-test\"\n",
    "TEST_DATA_SET_IMAGES_DIR_PATH = \"\"  # Replace with the path to your training images directory\n",
    "TEST_DATA_SET_ANN_FILE_PATH   = \"\"  # Replace with your test JSON file path\n",
    "\n",
    "register_coco_instances(\n",
    "    name=TEST_DATA_SET_NAME, \n",
    "    metadata={}, \n",
    "    json_file=TEST_DATA_SET_ANN_FILE_PATH, \n",
    "    image_root=TEST_DATA_SET_IMAGES_DIR_PATH\n",
    ")\n",
    "\n",
    "# VALIDATION SET - Manually specify the paths\n",
    "VALID_DATA_SET_NAME = f\"{DATA_SET_NAME}-valid\"\n",
    "VALID_DATA_SET_IMAGES_DIR_PATH = \"\" \n",
    "VALID_DATA_SET_ANN_FILE_PATH   = \"\"  # Replace with your test JSON file path\n",
    "\n",
    "register_coco_instances(\n",
    "    name=VALID_DATA_SET_NAME, \n",
    "    metadata={}, \n",
    "    json_file=VALID_DATA_SET_ANN_FILE_PATH, \n",
    "    image_root=VALID_DATA_SET_IMAGES_DIR_PATH\n",
    ")\n",
    "\n",
    "[\n",
    "    data_set\n",
    "    for data_set\n",
    "    in MetadataCatalog.list()\n",
    "    if data_set.startswith(DATA_SET_NAME)\n",
    "]\n",
    "metadata = MetadataCatalog.get(TRAIN_DATA_SET_NAME)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0de32d56-d652-467b-aaab-7ffc67629362",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load metadata and dataset\n",
    "metadata = MetadataCatalog.get(TRAIN_DATA_SET_NAME)\n",
    "dataset_train = DatasetCatalog.get(TRAIN_DATA_SET_NAME)\n",
    "\n",
    "# Load a single dataset entry\n",
    "dataset_entry = dataset_train[0]\n",
    "image = cv2.imread(dataset_entry[\"file_name\"])\n",
    "\n",
    "# Create visualizer instance\n",
    "visualizer = Visualizer(\n",
    "    image[:, :, ::-1],\n",
    "    metadata=metadata,\n",
    "    scale=0.8,\n",
    "    instance_mode=ColorMode.IMAGE_BW  # Optional: Black and white background\n",
    ")\n",
    "\n",
    "# Draw dataset dictionary\n",
    "out = visualizer.draw_dataset_dict(dataset_entry)\n",
    "\n",
    "# Display the image using matplotlib\n",
    "output_image = out.get_image()[:, :, ::-1]\n",
    "plt.figure(figsize=(12, 8))  # Adjust the figure size as needed\n",
    "plt.imshow(output_image)\n",
    "plt.axis(\"off\")  # Hide axis\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "920e26c8-32e3-4185-ba55-824a6a798e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HYPERPARAMETERS\n",
    "ARCHITECTURE = \"mask_rcnn_R_101_FPN_3x\"\n",
    "CONFIG_FILE_PATH = f\"COCO-InstanceSegmentation/{ARCHITECTURE}.yaml\"\n",
    "MAX_ITER = 2000\n",
    "EVAL_PERIOD = 200\n",
    "BASE_LR = 0.001\n",
    "NUM_CLASSES = 3\n",
    "# OUTPUT DIR\n",
    "OUTPUT_DIR_PATH = os.path.join(\n",
    "    DATA_SET_NAME, \n",
    "    ARCHITECTURE, \n",
    "    datetime.now().strftime('%Y-%m-%d-%H-%M-%S')\n",
    ")\n",
    "os.makedirs(OUTPUT_DIR_PATH, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0102246-3eb8-47e9-acf2-bb124cb6740a",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = get_cfg()\n",
    "cfg.merge_from_file(model_zoo.get_config_file(CONFIG_FILE_PATH))\n",
    "cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(CONFIG_FILE_PATH)\n",
    "cfg.DATASETS.TRAIN = (TRAIN_DATA_SET_NAME,)\n",
    "cfg.DATASETS.TEST = (TEST_DATA_SET_NAME,)\n",
    "cfg.MODEL.ROI_HEADS.BATCH_SIZE_PER_IMAGE = 64\n",
    "cfg.TEST.EVAL_PERIOD = EVAL_PERIOD\n",
    "cfg.DATALOADER.NUM_WORKERS = 2\n",
    "cfg.SOLVER.IMS_PER_BATCH = 2\n",
    "cfg.INPUT.MASK_FORMAT='bitmask'\n",
    "cfg.SOLVER.BASE_LR = BASE_LR\n",
    "cfg.SOLVER.MAX_ITER = MAX_ITER\n",
    "cfg.MODEL.ROI_HEADS.NUM_CLASSES = NUM_CLASSES\n",
    "cfg.OUTPUT_DIR = OUTPUT_DIR_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "463acf7b-61dc-4dff-90cc-b724ea283a46",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = DefaultTrainer(cfg) \n",
    "trainer.resume_or_load(resume=False)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "686d6680-fd53-49d2-8f4c-f76f662c57d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = get_cfg()\n",
    "ARCHITECTURE = \"mask_rcnn_R_101_FPN_3x\"\n",
    "# COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml\n",
    "CONFIG_FILE_PATH = cfg.merge_from_file(model_zoo.get_config_file(\"COCO-InstanceSegmentation/mask_rcnn_R_101_FPN_3x.yaml\"))\n",
    "cfg.MODEL.WEIGHTS = \"./leaf/mask_rcnn_R_101_FPN_3x/2025-02-01-10-12-17/model_final.pth\"  # Path to trained model\n",
    "cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.5  # Adjust threshold as needed\n",
    "cfg.MODEL.DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"  # Use GPU if available\n",
    "cfg.MODEL.ROI_HEADS.NUM_CLASSES = 1\n",
    "predictor = DefaultPredictor(cfg)\n",
    "\n",
    "num_samples = 20\n",
    "dataset_valid = DatasetCatalog.get(VALID_DATA_SET_NAME)\n",
    "metadata = MetadataCatalog.get(VALID_DATA_SET_NAME)\n",
    "dataset_valid = DatasetCatalog.get(VALID_DATA_SET_NAME)\n",
    "samples_to_visualize = dataset_valid[:num_samples]\n",
    "\n",
    "# Iterate through the samples and visualize\n",
    "for i, d in enumerate(samples_to_visualize):\n",
    "    img = cv2.imread(d[\"file_name\"])\n",
    "    outputs = predictor(img)\n",
    "    instances = outputs[\"instances\"].to(\"cpu\")\n",
    "    pred_masks = instances.pred_masks.numpy().astype(np.int32)  # Convert masks\n",
    "    pred_boxes = instances.pred_boxes.tensor.numpy().astype(np.int32)  # Convert boxes\n",
    "    pred_scores = instances.scores.numpy()  # Get confidence scores\n",
    "    pred_classes = instances.pred_classes.numpy()  # Get class labels\n",
    "    print(\"Image shape:\", img.shape)\n",
    "    print(\"pred_boxes shape:\", pred_masks.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
