{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Leaf segmentation inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install loguru\n",
    "!pip install gdown\n",
    "!pip install ftfy\n",
    "!pip install ultralytics scikit-learn opencv-python\n",
    "!pip install filterpy\n",
    "!python -m pip install 'git+https://github.com/facebookresearch/detectron2.git'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import cv2\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import imageio.v3 as iio\n",
    "from typing import List\n",
    "import torch, detectron2\n",
    "from datetime import datetime\n",
    "from detectron2.data.datasets import register_coco_instances\n",
    "from detectron2.data import DatasetCatalog, MetadataCatalog\n",
    "from detectron2.utils.visualizer import Visualizer\n",
    "from detectron2.utils.visualizer import ColorMode\n",
    "from detectron2 import model_zoo\n",
    "from detectron2.config import get_cfg\n",
    "from detectron2.engine import DefaultPredictor\n",
    "from detectron2.engine import DefaultTrainer\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from typing import Dict, Any"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inference parameters\n",
    "PARAMS = {\n",
    "    \"device\": \"cuda:1\",  # which device to use for inference\n",
    "    \"out_dir\": \"../results_stage4/\",  # directory to save results\n",
    "    \"save_leafs\": True,  # save the separate leafs images. Disable to make processing slightly faster\n",
    "    \"method\": \"final\",  # which method to use. Options: \"yolo\", \"final\" (yolo+sam)\n",
    "}\n",
    "\n",
    "# Data parameters \n",
    "\n",
    "# option 1.  \n",
    "DATA_PARAMS = {\n",
    "    \"images_root\": \"\",  # path to images\n",
    "    \"masks_root\": None,  # path to labels. Can be None, but without labels no metrics can be calculated\n",
    "}\n",
    "\n",
    "# option 2. \n",
    "DATA_PARAMS = {\n",
    "    \"images_root\": \"\",\n",
    "    \"masks_root\": \"\",\n",
    "    \"ds.csv\": \"../data_meta/ds.csv\",\n",
    "    \"nn_roles\": [\"test\"],\n",
    "    \"share\": 1.0,  # what share of sequences to process, use 1. to process all sequences\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Util part: imports, functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd /home/rsaric/Desktop/leaf_cv/notebooks\n",
    "sys.path.insert(0, \"../src\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import traceback\n",
    "\n",
    "import imageio\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import tqdm as orig_tqdm\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from dataset import load_or_build_dataset\n",
    "from metrics import (\n",
    "    AbstractMetric,\n",
    "    FrameBasedIOU,\n",
    "    MultiObjectTrackingAccuracy,\n",
    "    MultiObjectTrackingPrecision,\n",
    ")\n",
    "from saveload import EmptySaver, Saver, read_image, read_masks\n",
    "from masks import draw_joined_masks_on_image, mask_joined_to_masks_dict\n",
    "from model.tracking import ensure_same_image_sizes, change_mask_resolution\n",
    "\n",
    "\n",
    "class NoTqdm(orig_tqdm.tqdm):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        kwargs[\"disable\"] = True\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "orig_tqdm.tqdm = NoTqdm\n",
    "from model.yolo_models import (  # noqa E402,E501 # pylint: disable=wrong-import-position\n",
    "    YoloTrackerModel,\n",
    "    AbstractModel,\n",
    ")\n",
    "from model.final_model import (  # noqa E402,E501 # pylint: disable=wrong-import-position\n",
    "    VideoSAMFinal,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = get_cfg()\n",
    "cfg.merge_from_file(model_zoo.get_config_file(\"COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml\"))\n",
    "cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.5\n",
    "cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(\"COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml\")\n",
    "!wget http://images.cocodataset.org/val2017/000000439715.jpg -q -O input.jpg\n",
    "image = cv2.imread(\"./input.jpg\")\n",
    "predictor = DefaultPredictor(cfg)\n",
    "outputs = predictor(image)\n",
    "print(outputs[\"instances\"].pred_classes)\n",
    "print(outputs[\"instances\"].pred_boxes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subselect_sequences(\n",
    "    ds: pd.DataFrame,\n",
    "    nn_roles: list[str],\n",
    "    share: float = 1.0,\n",
    "    seed: int = 1,\n",
    "):\n",
    "    \"\"\"Select from dataset only set of sequences.\n",
    "\n",
    "    Args:\n",
    "        ds (pd.DataFrame): dataset\n",
    "        nn_roles (list[str]): roles to select\n",
    "        share (float): share of sequences to select\n",
    "        seed (int): random seed\n",
    "    \"\"\"\n",
    "    not_in_roles = set(nn_roles) - set(ds[\"nn_role\"].unique())\n",
    "    assert not not_in_roles, f\"Roles {not_in_roles} are not in the dataset\"\n",
    "\n",
    "    ds = ds[ds[\"nn_role\"].isin(nn_roles)].copy()\n",
    "    sequences_ids = ds[[\"plant\", \"rep\"]].drop_duplicates()\n",
    "    selected_sequences = sequences_ids.sample(frac=share, random_state=seed)\n",
    "    if share < 1:\n",
    "        print(selected_sequences.copy().sort_values(by=[\"plant\"]))\n",
    "        # print(\"selected_ids = \", selected_sequences.copy().set_index([\"plant\", \"rep\"]).index)\n",
    "\n",
    "    ds = ds[\n",
    "        ds.set_index([\"plant\", \"rep\"]).index.isin(\n",
    "            selected_sequences.set_index([\"plant\", \"rep\"]).index\n",
    "        )\n",
    "    ]\n",
    "    return ds\n",
    "\n",
    "def _model_inference_subdataset(\n",
    "    ds: pd.DataFrame,\n",
    "    model: AbstractModel,\n",
    "    metrics: list[AbstractMetric],\n",
    "    saver: Saver,\n",
    ") -> tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    for metric in metrics:\n",
    "        metric.reset()\n",
    "\n",
    "    nn_roles = ds[\"nn_role\"].unique()\n",
    "    assert len(nn_roles) == 1, f\"Multiple nn_roles in the dataset: {nn_roles}\"\n",
    "    description = f\"Inference for {nn_roles[0]} sequences ({len(ds)} frames)\"\n",
    "\n",
    "    for group, rep in tqdm(ds.groupby([\"plant\", \"rep\"]), desc=description):\n",
    "        try:\n",
    "            rep = rep.sort_values(\"image_num\")\n",
    "            images = []\n",
    "            masks_labels = []\n",
    "            for _, row in rep.iterrows():\n",
    "                images.append(read_image(row))\n",
    "\n",
    "                if len(metrics) > 0:\n",
    "                    # no parsing masks if metrics are disabled.\n",
    "                    masks_labels.append(read_masks(row))\n",
    "\n",
    "            # fix sizes:\n",
    "            images = ensure_same_image_sizes(images, f\"{group[0]}/{group[1]}\")\n",
    "            for img, masks in zip(images, masks_labels):\n",
    "                for v in masks.values():\n",
    "                    if v[\"segmentation\"].shape[:2] != img.shape[:2]:\n",
    "                        v[\"segmentation\"] = change_mask_resolution(\n",
    "                            v[\"segmentation\"], img.shape[:2]\n",
    "                        )\n",
    "\n",
    "            masks_predicted = model.predict_masks(images)\n",
    "            for i, (_, row) in enumerate(rep.iterrows()):\n",
    "                saver.save_masks(images[i], row, masks_predicted[i])\n",
    "\n",
    "            for metric in metrics:\n",
    "                # TODO: make it frame-based to avoid storing a lot in memory\n",
    "                metric.add_sequence(masks_labels, masks_predicted, name=\"/\".join(group))\n",
    "\n",
    "            saver.finalize_sequence(row)\n",
    "\n",
    "        except KeyboardInterrupt:\n",
    "            print(\"Inference interrupted\")\n",
    "            raise\n",
    "\n",
    "        except Exception:\n",
    "            traceback.print_exc()\n",
    "            print(f\"Error during inference for sequence {group}\")\n",
    "\n",
    "    seq_metrics = []\n",
    "    total_metrics = []\n",
    "    for metric in metrics:\n",
    "        seq_metrics.append(metric.get_aggregate_metrics(per_seq=True))\n",
    "        total_metrics.append(metric.get_aggregate_metrics(per_seq=False))\n",
    "        metric.reset()\n",
    "\n",
    "    if len(seq_metrics) > 0:\n",
    "        df_seq_metrics = pd.concat(seq_metrics, axis=1)\n",
    "        df_total_metrics = pd.concat(total_metrics, axis=1)\n",
    "    else:\n",
    "        df_seq_metrics = pd.DataFrame()\n",
    "        df_total_metrics = pd.DataFrame()\n",
    "    return df_seq_metrics, df_total_metrics\n",
    "\n",
    "def model_inference(\n",
    "    ds: pd.DataFrame,\n",
    "    model: AbstractModel,\n",
    "    metrics: list[AbstractMetric],\n",
    "    saver: Saver,\n",
    "):\n",
    "    \"\"\"Inference function.\n",
    "\n",
    "    Args:\n",
    "        ds (pd.DataFrame): dataset\n",
    "        out_dir (str): output directory\n",
    "        model (AbstractModel): model\n",
    "        metrics (list[AbstractMetric]): list of metrics to calculate\n",
    "        save_results (bool): save results to the disk\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: dataset with results\n",
    "    \"\"\"\n",
    "    saver.save_configs(model.config)\n",
    "    seq_results = []\n",
    "    total_results = []\n",
    "    for nn_role in [\"train\", \"val\", \"test\"]:\n",
    "        nn_role_ds = ds[ds[\"nn_role\"] == nn_role]\n",
    "        if len(nn_role_ds) == 0:\n",
    "            continue\n",
    "\n",
    "        seq_results_df, total_results_df = _model_inference_subdataset(\n",
    "            nn_role_ds, model, metrics, saver\n",
    "        )\n",
    "        seq_results_df[\"nn_role\"] = nn_role\n",
    "        seq_results.append(seq_results_df)\n",
    "        total_results_df[\"nn_role\"] = nn_role\n",
    "        total_results.append(total_results_df)\n",
    "\n",
    "    seq_dfs_joined = pd.concat(seq_results, axis=0)\n",
    "    total_dfs_joined = pd.concat(total_results, axis=0)\n",
    "    saver.save_metrics(seq_dfs_joined, per_seq=True)\n",
    "    saver.save_metrics(total_dfs_joined, per_seq=False)\n",
    "    return total_dfs_joined\n",
    "    \n",
    "def show_image_masks(image, masks, descriptions):\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.imshow(image)\n",
    "    plt.title(descriptions[0])\n",
    "\n",
    "    if masks is not None:\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.imshow(draw_joined_masks_on_image(image, masks, not_on_image=False))\n",
    "        plt.title(descriptions[1])\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 3. Prepare dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply default values if not specified:\n",
    "DATA_PARAMS[\"ds.csv\"] = DATA_PARAMS.get(\"ds.csv\", None)\n",
    "DATA_PARAMS[\"nn_roles\"] = DATA_PARAMS.get(\n",
    "    \"nn_roles\", [\"test\"]\n",
    ")  # \"test\" is used in case of not specified in ds.csv\n",
    "DATA_PARAMS[\"share\"] = DATA_PARAMS.get(\"share\", 1.0)\n",
    "PARAMS[\"save_results\"] = PARAMS.get(\n",
    "    \"save_results\", True\n",
    ")   \n",
    "PARAMS[\"seed\"] = PARAMS.get(\"seed\", 1)  # fix seed for image\n",
    "\n",
    "ds = load_or_build_dataset(\n",
    "    DATA_PARAMS[\"ds.csv\"], DATA_PARAMS[\"images_root\"], DATA_PARAMS[\"masks_root\"]\n",
    ")\n",
    "subselected_ds = subselect_sequences(\n",
    "    ds, DATA_PARAMS[\"nn_roles\"], DATA_PARAMS[\"share\"], PARAMS[\"seed\"]\n",
    ")\n",
    "\n",
    "def show_ds(name: str, ds: pd.DataFrame) -> None:\n",
    "    \"\"\"Show dataset statistics.\"\"\"\n",
    "    print(f\"{name} has\")\n",
    "    for nn_role in [\"test\"]:\n",
    "        role_ds = ds[ds[\"nn_role\"] == nn_role]\n",
    "        print(\n",
    "            f\"    {nn_role}  images: {len(role_ds)}, sequences: {len(role_ds.groupby(['plant', 'rep']))}\"\n",
    "        )\n",
    "    print()\n",
    "\n",
    "\n",
    "show_ds(\"Dataset\", ds)\n",
    "print(\n",
    "    f\"Subselected {DATA_PARAMS['share']} of {DATA_PARAMS['nn_roles']} sequences for inference\"\n",
    ")\n",
    "print()\n",
    "show_ds(\"Subselected dataset\", subselected_ds)\n",
    "random.seed(PARAMS[\"seed\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import imageio.v3 as iio\n",
    "from ultralytics import YOLO\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "from sklearn.metrics import jaccard_score\n",
    "from boxmot.appearance.reid_auto_backend import ReidAutoBackend\t\n",
    "from typing import Any, Tuple\n",
    "from collections import namedtuple\n",
    "\n",
    "# Load YOLOv8 model\n",
    "model_path = \"\"\n",
    "model = YOLO(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_yolo_model(model_path: str) -> YOLO:\n",
    "    \"\"\"\n",
    "    Load a YOLOv8 segmentation model from the specified path.\n",
    "\n",
    "    Args:\n",
    "        model_path (str): Path to the YOLOv8 model file.\n",
    "\n",
    "    Returns:\n",
    "        YOLO: Loaded YOLOv8 model.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        model = YOLO(model_path)  # Load the YOLOv8 segmentation model\n",
    "        print(f\"Model loaded successfully from: {model_path}\")\n",
    "        return model\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading the model from {model_path}: {e}\")\n",
    "        raise\n",
    "        \n",
    "# Define model paths\n",
    "model_paths = {\n",
    "    \"yolo_v8\": \"\",\n",
    "    \"yolo_v11\": \"\"\n",
    "}\n",
    "\n",
    "# Load models and store them in a dictionary\n",
    "models = {name: load_yolo_model(path) for name, path in model_paths.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.insert(0, \"../src\")\n",
    "SAM_PATH = \"../thirdparty/segment-anything-2/\"\n",
    "sys.path.insert(1, SAM_PATH)\n",
    "os.environ[\"HYDRA_FULL_ERROR\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import tempfile\n",
    "import torch\n",
    "from PIL import Image\n",
    "from ultralytics import YOLO\n",
    "from sam2.build_sam import build_sam2\n",
    "from sam2.sam2_image_predictor import SAM2ImagePredictor\n",
    "from pathlib import Path\n",
    "from trackers.strongsort.strongsort import StrongSort\n",
    "from trackers.deepocsort.deepocsort import DeepOcSort\n",
    "from trackers.bytetrack.bytetrack import ByteTrack\n",
    "\n",
    "class SAM2Inference:\n",
    "    def __init__(self, config: dict, device: str):\n",
    "        \"\"\"\n",
    "        Initialize the SAM2 model with the provided configuration.\n",
    "\n",
    "        Args:\n",
    "            config (dict): Configuration for SAM2 and YOLO.\n",
    "            device (str): Device to run inference (e.g., 'cuda' or 'cpu').\n",
    "        \"\"\"\n",
    "        self.config = config\n",
    "        self.device = torch.device(device)\n",
    "        self.yolo_model = YOLO(config[\"yolo_model\"]).to(self.device)\n",
    "        self.sam2_model = build_sam2(\n",
    "            config[\"sam2_cfg\"], config[\"sam2_checkpoint\"], device=self.device\n",
    "        )\n",
    "        self.sam2_predictor = SAM2ImagePredictor(self.sam2_model)\n",
    "\n",
    "    def infer_masks(self, image: np.ndarray) -> dict[int, dict]:\n",
    "        \"\"\"\n",
    "        Perform inference using YOLO for object detection and SAM2 for segmentation.\n",
    "\n",
    "        Args:\n",
    "            image (np.ndarray): Input image (H, W, C).\n",
    "\n",
    "        Returns:\n",
    "            dict[int, dict]: Predicted masks in SAM2 format.\n",
    "        \"\"\"\n",
    "        # Run YOLO detection\n",
    "        yolo_res = self.yolo_model(image, verbose=False)\n",
    "        over_threshold = yolo_res[0].boxes.xywh[\n",
    "            yolo_res[0].boxes.conf > self.config[\"yolo_threshold\"]\n",
    "        ]\n",
    "\n",
    "        if over_threshold.shape[0] == 0:\n",
    "            return {}  # No detections\n",
    "\n",
    "        points = over_threshold.cpu().detach().numpy()[:, None, :2]\n",
    "        labels = np.ones([len(points), 1])\n",
    "\n",
    "        # Run SAM2 prediction\n",
    "        self.sam2_predictor.set_image(image)\n",
    "        masks, scores, _ = self.sam2_predictor.predict(\n",
    "            point_coords=points,\n",
    "            point_labels=labels,\n",
    "            box=None,\n",
    "            multimask_output=False,\n",
    "        )\n",
    "        return {\n",
    "            idx: {\"segmentation\": masks[idx, 0, :, :] > 0.5}\n",
    "            for idx in range(masks.shape[0])\n",
    "        }\n",
    "\n",
    "def sam2_model(config: dict, device: str):\n",
    "    \"\"\"\n",
    "    Infer masks on a list of images using SAM2.\n",
    "\n",
    "    Args:\n",
    "        config (dict): Configuration for SAM2 and YOLO.\n",
    "        device (str): Device to run inference (e.g., 'cuda' or 'cpu').\n",
    "\n",
    "    Returns:\n",
    "        list[dict[int, dict]]: List of predicted masks for each image.\n",
    "    \"\"\"\n",
    "    sam2_inference = SAM2Inference(config, device)\n",
    "    return sam2_inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the list of trackers and their parameters\n",
    "trackers_config = [\n",
    "    {\n",
    "        \"name\": \"ByteTrack\",\n",
    "        \"params\": {\n",
    "            \"track_thresh\": 0.1,\n",
    "            \"match_thresh\": 0.8,\n",
    "            \"track_buffer\": 30,\n",
    "            \"frame_rate\": 30\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"DeepOcSort\",\n",
    "        \"params\": {\n",
    "            \"reid_weights\": Path(\"weights/lmbn_n_cuhk03_d.pt\"),\n",
    "            \"device\": \"cuda:1\",\n",
    "            \"half\": False,\n",
    "            \"per_class\": False,\n",
    "            \"det_thresh\": 0.2,\n",
    "            \"max_age\": 50,\n",
    "            \"min_hits\": 1,\n",
    "            \"iou_threshold\": 0.2,\n",
    "            \"delta_t\": 3,\n",
    "            \"asso_func\": \"iou\",\n",
    "            \"inertia\": 0.4,\n",
    "            \"w_association_emb\": 0.5,\n",
    "            \"alpha_fixed_emb\": 0.99\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"StrongSort\",\n",
    "        \"params\": {\n",
    "            \"reid_weights\": Path(\"weights/lmbn_n_cuhk03_d.pt\"),\n",
    "            \"device\": \"cuda:1\",\n",
    "            \"half\": False,\n",
    "            \"max_cos_dist\": 0.3,\n",
    "            \"max_iou_dist\": 0.9,\n",
    "            \"max_age\": 50,\n",
    "            \"n_init\": 1,\n",
    "            \"nn_budget\": 70,\n",
    "            \"mc_lambda\": 0.9,\n",
    "            \"ema_alpha\": 0.95\n",
    "        }\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_tracker(tracker_name, **kwargs):\n",
    "    \"\"\"\n",
    "    Initialize a tracker based on the specified name and parameters.\n",
    "\n",
    "    Args:\n",
    "        tracker_name (str): Name of the tracker. Options: 'ByteTrack', 'DeepOcSort', 'StrongSort'.\n",
    "        **kwargs: Tracker-specific parameters for customization.\n",
    "\n",
    "    Returns:\n",
    "        Tracker instance: The initialized tracker.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If the tracker name is not recognized.\n",
    "    \"\"\"\n",
    "    if tracker_name == \"ByteTrack\":\n",
    "        return ByteTrack(**kwargs)\n",
    "    elif tracker_name == \"DeepOcSort\":\n",
    "        return DeepOcSort(**kwargs)\n",
    "    elif tracker_name == \"StrongSort\":\n",
    "        return StrongSort(**kwargs)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown tracker name: {tracker_name}\")\n",
    "        \n",
    "def process_tracking_seg_masks(\n",
    "    image_files: List[str], model, tracker, generate_unique_color, mode: str = \"yolo\"\n",
    ") -> (np.ndarray, np.ndarray):\n",
    "    \"\"\"\n",
    "    Process a list of image files, generate tracking segmentation masks, stack them, and stack the resized images.\n",
    "\n",
    "    Args:\n",
    "        image_files (List[str]): List of paths to image files.\n",
    "        model: Model instance for object detection and segmentation (YOLO or SAM).\n",
    "        tracker: Tracker instance for tracking objects across frames.\n",
    "        generate_unique_color: Function to generate unique colors for track IDs.\n",
    "        mode (str): Mode of operation, either \"yolo\" or \"sam\".\n",
    "\n",
    "    Returns:\n",
    "        tuple: \n",
    "            - np.ndarray: Stacked tracking segmentation masks with uniform dimensions.\n",
    "            - np.ndarray: Stacked resized images with uniform dimensions.\n",
    "    \"\"\"\n",
    "    tracking_seg_masks_list = []\n",
    "    resized_images_list = []\n",
    "    track_colors: Dict[int, tuple] = {}  # To store track_id and corresponding colors\n",
    "    frame_count = 0  # Counter for frames\n",
    "\n",
    "    for image_file in image_files:\n",
    "        # Read the image\n",
    "        image = iio.imread(image_file)\n",
    "        # Resize the image to 640x640\n",
    "        image = cv2.resize(image, (640, 640))\n",
    "        resized_image = cv2.resize(image, (533, 517))\n",
    "        resized_images_list.append(resized_image)\n",
    "\n",
    "        if mode == \"yolo\":\n",
    "            # YOLO mode\n",
    "            model.conf = 0.15\n",
    "            results = model(image, verbose=False)\n",
    "            segmentation_mask = results[0].masks.data.cpu().numpy().astype(np.int32)  # Shape: (n, 640, 640)\n",
    "        elif mode == \"sam\":\n",
    "            # SAM mode\n",
    "            mask_dict = model.infer_masks(image)  # Assuming SAM model has `infer_masks`\n",
    "            segmentation_mask = np.stack(\n",
    "                [mask_data[\"segmentation\"] for mask_data in mask_dict.values()], axis=0\n",
    "            ).astype(np.int32)  # Shape: (n, 640, 640)\n",
    "\n",
    "            # Generate bounding boxes for SAM2 masks\n",
    "            dets = []\n",
    "            for mask_idx, mask in enumerate(segmentation_mask):\n",
    "                if mask.sum() > 0:  # Ensure the mask is not empty\n",
    "                    y_indices, x_indices = np.where(mask > 0)\n",
    "                    x1, y1, x2, y2 = x_indices.min(), y_indices.min(), x_indices.max(), y_indices.max()\n",
    "                    dets.append([x1, y1, x2, y2, 0.9, 1])  # Add bounding box with class=1 and confidence=0.9\n",
    "                print(\"dets\", dets)\n",
    "        elif mode == \"detectron2\":\n",
    "            # DETECTRON2 mode\n",
    "            outputs = model(image)  # Get predictions from Detectron2\n",
    "        \n",
    "            instances = outputs[\"instances\"].to(\"cpu\")\n",
    "            pred_masks = instances.pred_masks.numpy().astype(np.int32)  # Convert masks\n",
    "            pred_boxes = instances.pred_boxes.tensor.numpy().astype(np.int32)  # Convert boxes\n",
    "            pred_scores = instances.scores.numpy()  # Get confidence scores\n",
    "            pred_classes = instances.pred_classes.numpy()  # Get class labels\n",
    "            # print(\"Image shape:\", image.shape)\n",
    "            # print(\"pred_boxes shape:\", pred_boxes.shape)\n",
    "\n",
    "            # Convert to format similar to YOLO and SAM\n",
    "            segmentation_mask = pred_masks  # Shape: (n, height, width)\n",
    "            # print(segmentation_mask.shape)\n",
    "            # Generate bounding boxes\n",
    "            dets = []\n",
    "            # print(\"pred_masks\", pred_masks, \"pred_boxes\", pred_boxes)\n",
    "            if pred_masks.sum() > 0:\n",
    "                for i in range(len(pred_boxes)):\n",
    "                    x1, y1, x2, y2 = pred_boxes[i]\n",
    "                    score = pred_scores[i]\n",
    "                    class_id = pred_classes[i]\n",
    "                    dets.append([x1, y1, x2, y2, 0.9, 1])\n",
    "                # print(\"dets\", dets)\n",
    "        \n",
    "        else:\n",
    "            raise ValueError(\"Unsupported mode. Use 'yolo', 'sam', or 'detectron2'.\")\n",
    "        # Create tracking_seg with the same shape as segmentation_mask, initialized to 0\n",
    "        tracking_seg = np.zeros_like(segmentation_mask, dtype=np.int32)\n",
    "\n",
    "        if mode == \"yolo\":\n",
    "            # Prepare YOLO detections\n",
    "            dets = []\n",
    "            if len(results[0].boxes) > 0:  # Ensure there are detections\n",
    "                for det in results[0].boxes:\n",
    "                    x1, y1, x2, y2 = det.xyxy[0].tolist()\n",
    "                    conf = det.conf.item()  # Confidence score\n",
    "                    cls = int(det.cls.item())  # Class ID\n",
    "                    dets.append([x1, y1, x2, y2, conf, cls])\n",
    "\n",
    "        dets = np.array(dets) if dets else np.empty((0, 6))  # Ensure dets has the correct shape\n",
    "        tracks = tracker.update(dets, image)  # Update tracker\n",
    "\n",
    "        # print(tracks, tracks.shape)\n",
    "        # Assign class_id to the segmentation mask layer\n",
    "        for track in tracks:\n",
    "            x1, y1, x2, y2, track_id, score, cls, det_ind = track[:8]\n",
    "            print(\"x1, y1, x2, y2, track_id, score, cls, det_ind--\", x1, y1, x2, y2, track_id, score, cls, det_ind)\n",
    "            # Check if track_id already has a color\n",
    "            if track_id not in track_colors:\n",
    "                track_colors[track_id] = generate_unique_color(track_id)\n",
    "\n",
    "            # Start with the center point\n",
    "            center_x = int((x1 + x2) / 2)\n",
    "            center_y = int((y1 + y2) / 2)\n",
    "\n",
    "            # Check if the center point belongs to any layer\n",
    "            valid_mask_layer = None\n",
    "            for mask_idx, mask in enumerate(segmentation_mask):\n",
    "                if mask[center_y, center_x] > 0:\n",
    "                    valid_mask_layer = mask_idx\n",
    "                    break\n",
    "\n",
    "            if valid_mask_layer is not None:\n",
    "                # Assign track_id to the entire valid mask layer\n",
    "                tracking_seg[valid_mask_layer][segmentation_mask[valid_mask_layer] > 0] = track_id\n",
    "\n",
    "        # Resize tracking_seg to (517, 533)\n",
    "        resized_tracking_seg = np.array(\n",
    "            [cv2.resize(mask, (533, 517), interpolation=cv2.INTER_NEAREST) for mask in tracking_seg]\n",
    "        )\n",
    "        # print(\"resized_tracking_seg\", resized_tracking_seg)\n",
    "        # Append the resized tracking_seg to the list\n",
    "        tracking_seg_masks_list.append(resized_tracking_seg)\n",
    "        # print(\"tracking_seg_masks_list\", tracking_seg_masks_list)\n",
    "        frame_count += 1\n",
    "\n",
    "    # Determine the maximum number of layers across all masks\n",
    "    max_layers = max(mask.shape[0] for mask in tracking_seg_masks_list)\n",
    "\n",
    "    # Pad all masks to have the same number of layers\n",
    "    padded_tracking_seg_masks_list = []\n",
    "    for mask in tracking_seg_masks_list:\n",
    "        # print(\"ssss\", mask.shape)\n",
    "        num_layers = mask.shape[0]\n",
    "        if num_layers < max_layers:\n",
    "            # Pad with zeros\n",
    "            padding = np.zeros((max_layers - num_layers, mask.shape[1], mask.shape[2]), dtype=mask.dtype)\n",
    "            padded_mask = np.concatenate([mask, padding], axis=0)\n",
    "        else:\n",
    "            padded_mask = mask\n",
    "        padded_tracking_seg_masks_list.append(padded_mask)\n",
    "\n",
    "    # Stack all padded masks and resized images into single arrays\n",
    "    tracking_seg_masks = np.stack(padded_tracking_seg_masks_list, axis=0)\n",
    "    resized_images = np.stack(resized_images_list, axis=0)\n",
    "\n",
    "    return tracking_seg_masks, resized_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_ground_truth_masks(\n",
    "    mask_files: List[str],\n",
    "    tracking_seg_masks: np.ndarray,\n",
    "    target_size: tuple = (533, 517)\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Process and align ground truth masks to match the number of tracking segmentation mask layers.\n",
    "\n",
    "    Args:\n",
    "        mask_files (List[str]): List of file paths for ground truth masks.\n",
    "        tracking_seg_masks (np.ndarray): Tracking segmentation masks, shape (num_frames, num_layers, H, W).\n",
    "        target_size (tuple): Target size for resizing the ground truth masks, (width, height).\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Aligned ground truth masks with shape (num_frames, num_tracking_layers, H, W).\n",
    "    \"\"\"\n",
    "    # Initialize an empty list to store resized ground truth masks\n",
    "    ground_truth_list = []\n",
    "\n",
    "    # Step 1: Read, resize, and stack ground truth masks\n",
    "    for mask_file in tqdm(mask_files, desc=\"Reading and Resizing Ground Truth Masks\"):\n",
    "        mask = iio.imread(mask_file)  # Read the mask image\n",
    "        mask = cv2.resize(mask, target_size, interpolation=cv2.INTER_NEAREST)  # Resize to target size\n",
    "        ground_truth_list.append(mask)\n",
    "\n",
    "    # Combine all masks into a single 4D array\n",
    "    ground_truth = np.stack(ground_truth_list, axis=0)\n",
    "    # print(f\"Original ground_truth shape: {ground_truth.shape}\")  # Shape: (num_frames, H, W, C)\n",
    "\n",
    "    # Step 2: Extract the first layer (R channel) from the RGB ground truth\n",
    "    ground_truth_first_layer = ground_truth[:, :, :, 0]  # Shape: (num_frames, H, W)\n",
    "    # print(f\"Ground truth first layer shape: {ground_truth_first_layer.shape}\")\n",
    "\n",
    "    def create_unique_value_layers(gt_frame, num_layers):\n",
    "        \"\"\"\n",
    "        Create layers for each unique value in the ground truth frame, saving the unique value itself.\n",
    "        Args:\n",
    "            gt_frame (np.ndarray): Ground truth frame, shape (H, W).\n",
    "            num_layers (int): Number of layers to create (match with tracking_seg_masks).\n",
    "        Returns:\n",
    "            np.ndarray: Unique value layers, shape (num_layers, H, W).\n",
    "        \"\"\"\n",
    "        unique_values = np.unique(gt_frame)\n",
    "        unique_values = unique_values[unique_values > 0]  # Exclude background (0)\n",
    "        \n",
    "        layers = []\n",
    "        for val in unique_values:\n",
    "            value_layer = (gt_frame == val).astype(np.int32) * val  # Save the unique value in the layer\n",
    "            layers.append(value_layer)\n",
    "        \n",
    "        # Stack layers and pad with zeros to match num_layers\n",
    "        stacked_layers = np.stack(layers, axis=0)\n",
    "        if stacked_layers.shape[0] < num_layers:\n",
    "            padding = np.zeros((num_layers - stacked_layers.shape[0], *stacked_layers.shape[1:]), dtype=np.int32)\n",
    "            stacked_layers = np.vstack((stacked_layers, padding))\n",
    "        elif stacked_layers.shape[0] > num_layers:\n",
    "            stacked_layers = stacked_layers[:num_layers]\n",
    "        \n",
    "        return stacked_layers\n",
    "\n",
    "    # Step 4: Align layers for each frame\n",
    "    num_tracking_layers = tracking_seg_masks.shape[1]  # Number of layers in tracking_seg_masks\n",
    "    ground_truth_aligned = []\n",
    "\n",
    "    for frame_idx in tqdm(range(ground_truth_first_layer.shape[0]), desc=\"Aligning Ground Truth Layers\"):\n",
    "        aligned_layers = create_unique_value_layers(ground_truth_first_layer[frame_idx], num_tracking_layers)\n",
    "        ground_truth_aligned.append(aligned_layers)\n",
    "\n",
    "    # Ensure all frames have the same shape\n",
    "    max_num_layers = num_tracking_layers\n",
    "    final_ground_truth_aligned = []\n",
    "\n",
    "    for idx, aligned_layers in enumerate(ground_truth_aligned):\n",
    "        if aligned_layers.shape[0] < max_num_layers:\n",
    "            # Pad with zeros to ensure all frames have the same number of layers\n",
    "            padding = np.zeros((max_num_layers - aligned_layers.shape[0], *aligned_layers.shape[1:]), dtype=np.int32)\n",
    "            aligned_layers = np.vstack((aligned_layers, padding))\n",
    "        elif aligned_layers.shape[0] > max_num_layers:\n",
    "            # Trim excess layers if any\n",
    "            aligned_layers = aligned_layers[:max_num_layers]\n",
    "        \n",
    "        final_ground_truth_aligned.append(aligned_layers)\n",
    "\n",
    "    # Stack into a single array\n",
    "    ground_truth_aligned = np.stack(final_ground_truth_aligned, axis=0)\n",
    "    # print(f\"Final aligned ground_truth shape: {ground_truth_aligned.shape}\")  # Shape: (num_frames, num_tracking_layers, H, W)\n",
    "\n",
    "    return ground_truth_aligned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_tracking_performance_with_id_tracking(\n",
    "    ground_truth_aligned: np.ndarray, tracking_seg_masks: np.ndarray, overlap_threshold: float = 0.1\n",
    ") -> Dict[str, Any]:\n",
    "    \n",
    "    def calculate_iou(mask1: np.ndarray, mask2: np.ndarray) -> float:\n",
    "        \"\"\"Calculate Intersection over Union (IoU) between two masks.\"\"\"\n",
    "        intersection = np.logical_and(mask1, mask2).sum()\n",
    "        union = np.logical_or(mask1, mask2).sum()\n",
    "        return intersection / union if union > 0 else 0.0\n",
    "\n",
    "    def match_ids(gt_mask: np.ndarray, pred_mask: np.ndarray, threshold: float) -> Tuple[Dict[int, int], list[float]]:\n",
    "        \"\"\"Match ground truth IDs to predicted IDs based on IoU.\"\"\"\n",
    "        import matplotlib.pyplot as plt\n",
    "    \n",
    "        gt_ids = np.unique(gt_mask[gt_mask > 0])\n",
    "        pred_ids = np.unique(pred_mask[pred_mask > 0])\n",
    "        id_mapping = {}\n",
    "        ious = []\n",
    "        for gt_id in gt_ids:\n",
    "            gt_instance = gt_mask == gt_id\n",
    "    \n",
    "            best_iou = 0\n",
    "            best_pred_id = None\n",
    "            for pred_id in pred_ids:\n",
    "                pred_instance = pred_mask == pred_id\n",
    "    \n",
    "                iou = calculate_iou(gt_instance, pred_instance)\n",
    "    \n",
    "                if iou > best_iou and iou >= threshold:\n",
    "                    best_iou = iou\n",
    "                    best_pred_id = pred_id\n",
    "\n",
    "            if best_pred_id is not None:\n",
    "                id_mapping[gt_id] = best_pred_id\n",
    "                ious.append(best_iou)\n",
    "        return id_mapping, ious\n",
    "\n",
    "\n",
    "    def calculate_id_switches(prev_mapping: Dict[int, int], current_mapping: Dict[int, int]) -> int:\n",
    "        \"\"\"Calculate the number of ID switches between frames.\"\"\"\n",
    "        id_switches = 0\n",
    "        for gt_id, pred_id in current_mapping.items():\n",
    "            if gt_id in prev_mapping and prev_mapping[gt_id] != pred_id:\n",
    "                id_switches += 1\n",
    "        return id_switches\n",
    "\n",
    "    # Flatten the masks by taking the max ID across the instance dimension\n",
    "    gt_masks = np.max(ground_truth_aligned, axis=1)\n",
    "    pred_masks = np.max(tracking_seg_masks, axis=1)\n",
    "\n",
    "    total_tp, total_fp, total_fn, total_id_switches = 0, 0, 0, 0\n",
    "    total_ious = []\n",
    "    frame_ious = []\n",
    "    ground_truth_count = 0\n",
    "    prev_id_mapping = {}\n",
    "\n",
    "    for frame_idx in range(gt_masks.shape[0]):\n",
    "        gt_mask = gt_masks[frame_idx]\n",
    "        pred_mask = pred_masks[frame_idx]\n",
    "\n",
    "        # Match IDs between ground truth and predicted masks\n",
    "        current_id_mapping, ious = match_ids(gt_mask, pred_mask, overlap_threshold)\n",
    "\n",
    "        # Calculate ID switches\n",
    "        if frame_idx > 0:\n",
    "            total_id_switches += calculate_id_switches(prev_id_mapping, current_id_mapping)\n",
    "        prev_id_mapping = current_id_mapping\n",
    "\n",
    "        # Evaluate frame metrics\n",
    "        gt_ids   = np.unique(gt_mask[gt_mask > 0])\n",
    "        pred_ids = np.unique(pred_mask[pred_mask > 0])\n",
    "        tp = sum(1 for gt_id in gt_ids if gt_id in current_id_mapping)\n",
    "        # print(len(pred_ids), tp, len(gt_ids), \"=================\")\n",
    "\n",
    "        # fp = len(pred_ids) - tp\n",
    "        fp = max(len(pred_ids) - tp, 0)\n",
    "        fn = len(gt_ids) - tp\n",
    "    \n",
    "        total_tp += tp\n",
    "        total_fp += fp\n",
    "        total_fn += fn\n",
    "        total_ious.extend(ious)\n",
    "        ground_truth_count += len(gt_ids)\n",
    "\n",
    "        # Compute IoU for the entire frame\n",
    "        frame_iou = calculate_iou(gt_mask > 0, pred_mask > 0)  # Non-zero areas only\n",
    "        frame_ious.append(frame_iou)\n",
    "\n",
    "    # Calculate metrics\n",
    "    precision = total_tp / (total_tp + total_fp) if total_tp + total_fp > 0 else 0.0\n",
    "    recall = total_tp / (total_tp + total_fn) if total_tp + total_fn > 0 else 0.0\n",
    "    mota = 1 - (total_fn + total_fp + total_id_switches) / ground_truth_count if ground_truth_count > 0 else 0.0\n",
    "    motp = sum(total_ious) / len(total_ious) if total_ious else 0.0\n",
    "    frame_based_iou = sum(frame_ious) / len(frame_ious) if frame_ious else 0.0\n",
    "\n",
    "    metrics = {\n",
    "        \"GroundTruthMasksCount\": ground_truth_count,\n",
    "        \"MultiObjectTrackingAccuracy\": mota,\n",
    "        \"IDSwitches\": total_id_switches,\n",
    "        \"MultiObjectTrackingPrecision\": motp,\n",
    "        \"FalseNegatives\": total_fn,\n",
    "        \"FalsePositives\": total_fp,\n",
    "        \"FrameBasedIOU\": frame_based_iou,\n",
    "    }\n",
    "\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image_and_mask_files_from_dataset(image_paths, mask_paths):\n",
    "    \"\"\"\n",
    "    Retrieves and sorts image and mask files directly from dataset paths.\n",
    "    Args:\n",
    "        image_paths (list): List of image paths from the dataset.\n",
    "        mask_paths (list): List of mask paths from the dataset.\n",
    "    Returns:\n",
    "        tuple: Sorted lists of image and mask paths.\n",
    "    \"\"\"\n",
    "    image_files = sorted(image_paths)\n",
    "    mask_files = sorted(mask_paths)\n",
    "    return image_files, mask_files\n",
    "\n",
    "# Iterate over the processed data\n",
    "def iterate_and_return_files(processed_data):\n",
    "    \"\"\"\n",
    "    Iterates over processed data to retrieve sorted image and mask files.\n",
    "    Args:\n",
    "        processed_data (dict): Processed dataset organized by plant and rep.\n",
    "    Yields:\n",
    "        dict: Contains plant name, rep name, image files, and mask files.\n",
    "    \"\"\"\n",
    "    for plant, reps in processed_data.items():\n",
    "        for rep, paths in reps.items():\n",
    "            # Retrieve sorted image and mask files from dataset\n",
    "            image_files, mask_files = get_image_and_mask_files_from_dataset(\n",
    "                paths[\"image_paths\"], paths[\"mask_paths\"]\n",
    "            )\n",
    "            yield {\n",
    "                \"plant_name\": plant,\n",
    "                \"rep_name\": rep,\n",
    "                \"image_files\": image_files,\n",
    "                \"mask_files\": mask_files\n",
    "            }\n",
    "\n",
    "def iterate_and_return_files_with_validation(processed_data):\n",
    "    \"\"\"\n",
    "    Iterates over processed data, retrieves sorted and validated image and mask files.\n",
    "    Args:\n",
    "        processed_data (dict): Processed dataset organized by plant and rep.\n",
    "    Yields:\n",
    "        dict: Contains plant name, rep name, validated image files, and mask files.\n",
    "    \"\"\"\n",
    "    for plant, reps in processed_data.items():\n",
    "        for rep, paths in reps.items():\n",
    "            print(images_dir,masks_dir)\n",
    "            # Retrieve and sort image and mask files from dataset\n",
    "            image_files = sorted([os.path.join(images_dir, f) for f in os.listdir(images_dir) if f.endswith('.png')])\n",
    "            mask_files = sorted([os.path.join(masks_dir, f) for f in os.listdir(masks_dir) if f.endswith('.png')])\n",
    "            print(len(image_files))\n",
    "            # Ensure image and mask files are aligned\n",
    "            assert len(image_files) == len(mask_files), f\"Number of images and masks do not match for plant {plant}, rep {rep}!\"\n",
    "            for img, mask in zip(image_files, mask_files):\n",
    "                assert os.path.basename(img) == os.path.basename(mask), f\"Image and mask filenames do not align for plant {plant}, rep {rep}!\"\n",
    "            \n",
    "            yield {\n",
    "                \"plant_name\": plant,\n",
    "                \"rep_name\": rep,\n",
    "                \"image_files\": image_files,\n",
    "                \"mask_files\": mask_files\n",
    "            }\n",
    "\n",
    "# Iterate over the plants\n",
    "def process_plants(dataframe):\n",
    "    result = {}\n",
    "    for plant, plant_group in dataframe.groupby('plant'):\n",
    "        result[plant] = {}\n",
    "        # Iterate over the reps within each plant\n",
    "        for rep, rep_group in plant_group.groupby('rep'):\n",
    "            # Sort the data within each rep based on 'image_num'\n",
    "            sorted_rep_group = rep_group.sort_values(by='image_num')\n",
    "            # Collect image and mask paths\n",
    "            image_paths = sorted_rep_group['image_path'].tolist()\n",
    "            mask_paths = sorted_rep_group['mask_path'].tolist()\n",
    "            # Store the result for the current rep\n",
    "            result[plant][rep] = {\n",
    "                \"image_paths\": image_paths,\n",
    "                \"mask_paths\": mask_paths\n",
    "            }\n",
    "    return result\n",
    "\n",
    "def iterate_and_return_files_with_validation(processed_data):\n",
    "    \"\"\"\n",
    "    Iterates over processed data, retrieves sorted and validated image and mask files.\n",
    "    Args:\n",
    "        processed_data (dict): Processed dataset organized by plant and rep.\n",
    "    Yields:\n",
    "        dict: Contains plant name, rep name, validated and sorted image and mask files.\n",
    "    \"\"\"\n",
    "    for plant, reps in processed_data.items():\n",
    "        for rep, data in reps.items():\n",
    "            # Extract image_paths and mask_paths\n",
    "            image_paths = data.get('image_paths', [])\n",
    "            mask_paths = data.get('mask_paths', [])\n",
    "            \n",
    "            # Sort image_paths and mask_paths\n",
    "            image_paths = sorted(image_paths)\n",
    "            mask_paths = sorted(mask_paths)\n",
    "            \n",
    "            # Ensure the number of image and mask files match\n",
    "            assert len(image_paths) == len(mask_paths), (\n",
    "                f\"Mismatch in the number of images and masks for plant '{plant}', rep '{rep}'!\"\n",
    "            )\n",
    "            \n",
    "            # Ensure image and mask filenames match\n",
    "            for img, mask in zip(image_paths, mask_paths):\n",
    "                assert os.path.basename(img) == os.path.basename(mask), (\n",
    "                    f\"Image and mask filenames do not match for plant '{plant}', rep '{rep}': \"\n",
    "                    f\"{os.path.basename(img)} vs {os.path.basename(mask)}\"\n",
    "                )\n",
    "            \n",
    "            # Yield the sorted and validated data\n",
    "            yield {\n",
    "                \"plant_name\": plant,\n",
    "                \"rep_name\": rep,\n",
    "                \"image_files\": image_paths,\n",
    "                \"mask_files\": mask_paths\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import tempfile\n",
    "import numpy as np\n",
    "import torch\n",
    "from PIL import Image\n",
    "from ultralytics import YOLO\n",
    "from sam2.build_sam import build_sam2\n",
    "from sam2.sam2_image_predictor import SAM2ImagePredictor\n",
    "from detectron2.engine import DefaultPredictor\n",
    "from detectron2.checkpoint import DetectionCheckpointer\n",
    "from detectron2.config import get_cfg\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import pathlib\n",
    "import random\n",
    "import tempfile\n",
    "import numpy as np\n",
    "import torch\n",
    "from PIL import Image\n",
    "from ultralytics import YOLO\n",
    "from sam2.build_sam import build_sam2\n",
    "from sam2.sam2_image_predictor import SAM2ImagePredictor\n",
    "\n",
    "\n",
    "sys.path.insert(0, \"../src\")\n",
    "SAM_PATH = \"../thirdparty/segment-anything-2/\"\n",
    "sys.path.insert(1, SAM_PATH)\n",
    "os.environ[\"HYDRA_FULL_ERROR\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_yolo_model(model_path: str) -> YOLO:\n",
    "    \"\"\"\n",
    "    Load a YOLOv8 segmentation model from the specified path.\n",
    "\n",
    "    Args:\n",
    "        model_path (str): Path to the YOLOv8 model file.\n",
    "\n",
    "    Returns:\n",
    "        YOLO: Loaded YOLOv8 model.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        model = YOLO(model_path)  # Load the YOLOv8 segmentation model\n",
    "        print(f\"Model loaded successfully from: {model_path}\")\n",
    "        return model\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading the model from {model_path}: {e}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_detectron2_model(model_path: str, num_classes: int = 3, score_thresh: float = 0.7):\n",
    "    \"\"\"\n",
    "    Load a Detectron2 model from a given checkpoint path.\n",
    "\n",
    "    Args:\n",
    "        model_path (str): Path to the model checkpoint (.pth file).\n",
    "        config_path (str): Path to the model's config.yaml file.\n",
    "        num_classes (int): Number of classes in the dataset.\n",
    "        score_thresh (float): Score threshold for predictions.\n",
    "\n",
    "    Returns:\n",
    "        DefaultPredictor: A Detectron2 predictor object.\n",
    "    \"\"\"\n",
    "    cfg = get_cfg()\n",
    "    ARCHITECTURE = \"mask_rcnn_R_101_FPN_3x\"\n",
    "    # COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml\n",
    "    CONFIG_FILE_PATH = cfg.merge_from_file(model_zoo.get_config_file(\"COCO-InstanceSegmentation/mask_rcnn_R_101_FPN_3x.yaml\"))\n",
    "    cfg.MODEL.WEIGHTS = \"./leaf/mask_rcnn_R_101_FPN_3x/2025-02-01-10-12-17/model_final.pth\"  # Path to trained model\n",
    "    cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.5  # Adjust threshold as needed\n",
    "    cfg.MODEL.DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"  # Use GPU if available\n",
    "    cfg.MODEL.ROI_HEADS.NUM_CLASSES = 1\n",
    "\n",
    "    # Initialize predictor\n",
    "    predictor = DefaultPredictor(cfg)\n",
    "    return predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sys.path.insert(0, \"../src\")\n",
    "SAM_PATH = \"../thirdparty/segment-anything-2/\"\n",
    "sys.path.insert(1, SAM_PATH)\n",
    "os.environ[\"HYDRA_FULL_ERROR\"] = \"1\"\n",
    "\n",
    "# Define model paths\n",
    "model_paths = {\n",
    "    \"yolo_v8\": \"\",\n",
    "    \"yolo_v11\": \"\",\n",
    "    \"detectron2\": \"\"\n",
    "}\n",
    "\n",
    "# Define SAM2 configuration\n",
    "# Configuration for YOLO and SAM2\n",
    "config = {\n",
    "    \"yolo_model\": \"../models/yolo_11mseg_finetuned_stage4.pt\",\n",
    "    \"sam2_cfg\": \"configs/sam2.1/sam2.1_hiera_b+.yaml\",\n",
    "    \"sam2_checkpoint\": \"../models/sam2.1l_finetuned.pt\",\n",
    "    \"yolo_threshold\": 0.5,\n",
    "}\n",
    "\n",
    "device = \"cuda:1\" if torch.cuda.is_available() else \"cpu\"\n",
    "# Load models and store them in a dictionary\n",
    "models = {\n",
    "    \"yolo_v8\": load_yolo_model(model_paths[\"yolo_v8\"]),\n",
    "    \"yolo_v11\": load_yolo_model(model_paths[\"yolo_v11\"]),\n",
    "    \"sam2\": sam2_model(config, device),\n",
    "    \"detectron2\": load_detectron2_model(model_path=model_paths[\"detectron2\"])\n",
    "}\n",
    "\n",
    "# Print loaded models\n",
    "print(\"Loaded models:\")\n",
    "for model_name in models:\n",
    "    print(f\"- {model_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predefined color mapping\n",
    "PREDEFINED_COLORS = {\n",
    "    1:  (244, 64, 14),\n",
    "    2:  (48, 57, 249),\n",
    "    3:  (234, 250, 37),\n",
    "    4:  (24, 193, 65),\n",
    "    5:  (245, 130, 49),\n",
    "    6:  (231, 80, 219),\n",
    "    7:  (0, 182, 173),\n",
    "    8:  (115, 0, 218),\n",
    "    9:  (191, 239, 69),\n",
    "    10: (255, 250, 200),\n",
    "    11: (250, 190, 212),\n",
    "    12: (66, 212, 244),\n",
    "    13: (155, 99, 36),\n",
    "    14: (220, 190, 255),\n",
    "    15: (69, 158, 220),\n",
    "    16: (255, 216, 177),\n",
    "    17: (98, 2, 37),\n",
    "    18: (227, 213, 12),\n",
    "    19: (79, 159, 83),\n",
    "    20: (170, 23, 101),\n",
    "    21: (170, 255, 195),\n",
    "    22: (169, 169, 169),\n",
    "    23: (181, 111, 119),\n",
    "    24: (144, 121, 171),\n",
    "    25: (9, 125, 244),\n",
    "    26: (184, 70, 30),\n",
    "    27: (154, 35, 246),\n",
    "    28: (229, 225, 238),\n",
    "    29: (141, 254, 82),\n",
    "    30: (31, 200, 209),\n",
    "    31: (194, 217, 105),\n",
    "    32: (91, 20, 124),\n",
    "    33: (181, 220, 171),\n",
    "    34: (37, 3, 193),\n",
    "}\n",
    "\n",
    "# Cache for dynamically generated colors\n",
    "unique_color_mapping = {}\n",
    "\n",
    "def generate_unique_color(track_id):\n",
    "    \"\"\"\n",
    "    Generate a unique color for a track_id or retrieve it from predefined or dynamic mapping.\n",
    "\n",
    "    Args:\n",
    "        track_id (int): ID for which to generate or retrieve a color.\n",
    "\n",
    "    Returns:\n",
    "        tuple: BGR color as a tuple.\n",
    "    \"\"\"\n",
    "    # Check if color is predefined\n",
    "    if track_id in PREDEFINED_COLORS:\n",
    "        return PREDEFINED_COLORS[track_id]\n",
    "    \n",
    "    # Check if color is already generated dynamically\n",
    "    if track_id in unique_color_mapping:\n",
    "        return unique_color_mapping[track_id]\n",
    "    \n",
    "    # Generate a new random color\n",
    "    random.seed(track_id)  # Ensure consistent color generation for the same ID\n",
    "    color_rgb = np.random.randint(100, 255, 3)  # Generate a random RGB color\n",
    "    color_bgr = tuple(map(int, color_rgb[::-1]))  # Convert to BGR format for OpenCV compatibility\n",
    "    unique_color_mapping[track_id] = color_bgr\n",
    "    \n",
    "    return color_bgr\n",
    "\n",
    "def save_visualization(image_path, mask_path, frame_index, max_mask, unique_colors, original_image):\n",
    "    \"\"\"\n",
    "    Save the visualization of the mask with unique colors for each instance overlaid on the original image.\n",
    "\n",
    "    Parameters:\n",
    "        image_path (Path): Path to save the visualization image.\n",
    "        frame_index (int): Frame index for reference.\n",
    "        max_mask (ndarray): The reduced mask for the current frame (2D array).\n",
    "        unique_colors (dict): Dictionary mapping instance IDs to BGR colors.\n",
    "        original_image (ndarray): The original BGR image (3D array).\n",
    "        output_folder (str): Path to the folder where the overlaid images will be saved.\n",
    "    \"\"\"\n",
    "    # Create a blank RGB image for the mask visualization\n",
    "    height, width = max_mask.shape\n",
    "    visualization = np.zeros((height, width, 3), dtype=np.uint8)\n",
    "\n",
    "    # Apply colors to the mask\n",
    "    for instance_id, color in unique_colors.items():\n",
    "        visualization[max_mask == instance_id] = color\n",
    "\n",
    "    # Save the mask visualization\n",
    "    cv2.imwrite(str(mask_path), visualization)\n",
    "    visualization = cv2.cvtColor(visualization, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # Overlay the mask on the original image\n",
    "    overlayed_image = original_image.copy()\n",
    "    non_black_pixels = max_mask > 0  # Mask non-zero pixels\n",
    "    overlayed_image[non_black_pixels] = visualization[non_black_pixels]\n",
    "\n",
    "    # Convert the overlayed image to RGB format\n",
    "    overlayed_image_rgb = cv2.cvtColor(overlayed_image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # Save the overlayed image in RGB\n",
    "    cv2.imwrite(str(image_path), overlayed_image_rgb)\n",
    "\n",
    "\n",
    "def save_instance_masks(tracking_seg_masks, original_images, output_dir):\n",
    "    \"\"\"\n",
    "    Save instance masks into folders named leaf_{id}, where IDs are remapped to start from 1 and sorted.\n",
    "\n",
    "    Args:\n",
    "        tracking_seg_masks (numpy.ndarray): Instance segmentation masks with shape (frames, layers, H, W).\n",
    "        original_images (numpy.ndarray): Original images with shape (frames, H, W, 3).\n",
    "        output_dir (str): Parent directory to save instance mask folders.\n",
    "    \"\"\"\n",
    "    output_dir = Path(output_dir)\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Combine layers for all frames to get unique IDs and remap them\n",
    "    unique_ids = sorted(set(np.unique(tracking_seg_masks)) - {0})  # Remove background (ID=0) and sort\n",
    "    id_mapping = {old_id: new_id for new_id, old_id in enumerate(unique_ids, start=1)}  # Map old IDs to new IDs\n",
    "\n",
    "    # Ensure each ID folder exists\n",
    "    for new_id in id_mapping.values():\n",
    "        (output_dir / \"leaf_binary_masks\"/ f\"leaf_{new_id}\").mkdir(parents=True, exist_ok=True)\n",
    "        (output_dir / \"leaf_extracted_images\"/ f\"leaf_{new_id}\").mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Iterate over frames\n",
    "    for frame_idx, (frame_masks, original_image) in enumerate(zip(tracking_seg_masks, original_images)):\n",
    "        # Sum layers to get a single mask for the frame (max or sum per pixel to combine layers)\n",
    "        combined_frame_mask = frame_masks.max(axis=0)  # Shape: (H, W)\n",
    "\n",
    "        # Process each instance in the frame\n",
    "        for old_id, new_id in id_mapping.items():\n",
    "            # Extract binary mask for the current instance in this frame\n",
    "            instance_mask = (combined_frame_mask == old_id).astype(np.uint8)  # Binary mask (H, W)\n",
    "            if instance_mask.sum() == 0:\n",
    "                continue  # Skip if no instance in this frame\n",
    "\n",
    "            # Save black-and-white mask\n",
    "            bw_path = output_dir /  \"leaf_binary_masks\" / f\"leaf_{new_id}\" / f\"frame_{frame_idx:04d}.png\"\n",
    "            cv2.imwrite(str(bw_path), instance_mask * 255)  # Convert binary mask to 255 for saving\n",
    "\n",
    "            # Create an extracted RGB image with the instance region\n",
    "            extracted_image = np.zeros_like(original_image)  # Black background\n",
    "            extracted_image[instance_mask > 0] = original_image[instance_mask > 0]  # Copy region of interest\n",
    "\n",
    "            # Save RGB overlay\n",
    "            rgb_path = output_dir / \"leaf_extracted_images\" / f\"leaf_{new_id}\" / f\"frame_{frame_idx:04d}.png\"\n",
    "            cv2.imwrite(str(rgb_path), extracted_image)\n",
    "\n",
    "def process_tracking_custom_seg_masks(\n",
    "    image_files: List[str], model, tracker, generate_unique_color, mode: str = \"yolo\"\n",
    ") -> (np.ndarray, np.ndarray):\n",
    "    \"\"\"\n",
    "    Process a list of image files, generate tracking segmentation masks, stack them, and stack the resized images.\n",
    "\n",
    "    Args:\n",
    "        image_files (List[str]): List of paths to image files.\n",
    "        model: Model instance for object detection and segmentation (YOLO or SAM).\n",
    "        tracker: Tracker instance for tracking objects across frames.\n",
    "        generate_unique_color: Function to generate unique colors for track IDs.\n",
    "        mode (str): Mode of operation, either \"yolo\" or \"sam\".\n",
    "\n",
    "    Returns:\n",
    "        tuple: \n",
    "            - np.ndarray: Stacked tracking segmentation masks with uniform dimensions.\n",
    "            - np.ndarray: Stacked resized images with uniform dimensions.\n",
    "    \"\"\"\n",
    "    tracking_seg_masks_list = []\n",
    "    resized_images_list = []\n",
    "    track_colors: Dict[int, tuple] = {}  # To store track_id and corresponding colors\n",
    "    frame_count = 0  # Counter for frames\n",
    "\n",
    "    for image_file in image_files:\n",
    "        # Read the image\n",
    "        image = iio.imread(image_file)\n",
    "        # Resize the image to 640x640\n",
    "        image = cv2.resize(image, (640, 640))\n",
    "        resized_image = cv2.resize(image, (533, 517))\n",
    "        resized_images_list.append(resized_image)\n",
    "\n",
    "        if mode == \"detectron2\":\n",
    "            # DETECTRON2 mode\n",
    "            outputs = model(image)  # Get predictions from Detectron2\n",
    "            instances = outputs[\"instances\"].to(\"cpu\")\n",
    "            pred_masks = instances.pred_masks.numpy().astype(np.int32)  # Convert masks\n",
    "            pred_boxes = instances.pred_boxes.tensor.numpy().astype(np.int32)  # Convert boxes\n",
    "            pred_scores = instances.scores.numpy()  # Get confidence scores\n",
    "            pred_classes = instances.pred_classes.numpy()  # Get class labels\n",
    "            # print(\"Image shape:\", image.shape)\n",
    "            # print(\"pred_boxes shape:\", pred_boxes.shape)\n",
    "\n",
    "            # Convert to format similar to YOLO and SAM\n",
    "            segmentation_mask = pred_masks  # Shape: (n, height, width)\n",
    "            # print(segmentation_mask.shape)\n",
    "            # Generate bounding boxes\n",
    "            dets = []\n",
    "            # print(\"pred_masks\", pred_masks, \"pred_boxes\", pred_boxes)\n",
    "            if pred_masks.sum() > 0:\n",
    "                for i in range(len(pred_boxes)):\n",
    "                    x1, y1, x2, y2 = pred_boxes[i]\n",
    "                    score = pred_scores[i]\n",
    "                    class_id = pred_classes[i]\n",
    "                    dets.append([x1, y1, x2, y2, 0.9, 1])\n",
    "                # print(\"dets\", dets)\n",
    "        \n",
    "        else:\n",
    "            raise ValueError(\"Unsupported mode. Use 'yolo', 'sam', or 'detectron2'.\")\n",
    "        # Create tracking_seg with the same shape as segmentation_mask, initialized to 0\n",
    "        tracking_seg = np.zeros_like(segmentation_mask, dtype=np.int32)\n",
    "\n",
    "            # Get segmentation masks from the model\n",
    "        outputs = model_detectron2(image)\n",
    "        instances = outputs[\"instances\"].to(\"cpu\")\n",
    "        pred_masks = instances.pred_masks.numpy().astype(np.int32)  # Convert masks\n",
    "        pred_boxes = instances.pred_boxes.tensor.numpy().astype(np.int32)  # Convert boxes\n",
    "        pred_scores = instances.scores.numpy()  # Get confidence scores\n",
    "        pred_classes = instances.pred_classes.numpy()  # Get class labels\n",
    "        \n",
    "        # Convert to format similar to YOLO and SAM\n",
    "        segmentation_mask = pred_masks  # Shape: (n, height, width)\n",
    "        \n",
    "        dets = []\n",
    "        if pred_masks.sum() > 0:\n",
    "            for i in range(len(pred_boxes)):\n",
    "                x1, y1, x2, y2 = pred_boxes[i]\n",
    "                score = pred_scores[i]\n",
    "                class_id = pred_classes[i]\n",
    "                dets.append([x1, y1, x2, y2, 0.9, 1])\n",
    "                \n",
    "        # Convert masks into the tracker's format\n",
    "        detections = [{\"mask\": mask} for mask in segmentation_mask]\n",
    "        detections = np.array(detections)\n",
    "  \n",
    "        # Update the tracker\n",
    "        tracked_objects = tracker.update(detections)\n",
    "\n",
    "        tracks = []\n",
    "        \n",
    "        for key, obj in tracked_objects.items():\n",
    "            binary_image = obj['mask']  # Extract the binary mask\n",
    "            track_id = obj['id']  # Extract the track ID\n",
    "        \n",
    "            # Find non-zero pixels\n",
    "            non_zero_pixels = cv2.findNonZero(binary_image.astype(np.uint8))\n",
    "        \n",
    "            if non_zero_pixels is not None:\n",
    "                # Compute bounding box (x, y, width, height)\n",
    "                x, y, w, h = cv2.boundingRect(non_zero_pixels)\n",
    "        \n",
    "                # Convert to x1, y1, x2, y2\n",
    "                x1, y1 = x, y\n",
    "                x2, y2 = x + w, y + h\n",
    "        \n",
    "                # Append to tracks list\n",
    "                tracks.append([x1, y1, x2, y2, track_id, 1, 1, 0])\n",
    "        \n",
    "        for track in tracks:\n",
    "            x1, y1, x2, y2, track_id, score, cls, det_ind = track[:8]\n",
    "            # print(\"x1, y1, x2, y2, track_id, score, cls, det_ind--\", x1, y1, x2, y2, track_id, score, cls, det_ind)\n",
    "            # Check if track_id already has a color\n",
    "            if track_id not in track_colors:\n",
    "                track_colors[track_id] = generate_unique_color(track_id)\n",
    "\n",
    "            # Start with the center point\n",
    "            center_x = int((x1 + x2) / 2)\n",
    "            center_y = int((y1 + y2) / 2)\n",
    "\n",
    "            # Check if the center point belongs to any layer\n",
    "            valid_mask_layer = None\n",
    "            for mask_idx, mask in enumerate(segmentation_mask):\n",
    "                if mask[center_y, center_x] > 0:\n",
    "                    valid_mask_layer = mask_idx\n",
    "                    break\n",
    "\n",
    "            if valid_mask_layer is not None:\n",
    "                # Assign track_id to the entire valid mask layer\n",
    "                tracking_seg[valid_mask_layer][segmentation_mask[valid_mask_layer] > 0] = track_id\n",
    "\n",
    "        # Resize tracking_seg to (517, 533)\n",
    "        resized_tracking_seg = np.array(\n",
    "            [cv2.resize(mask, (533, 517), interpolation=cv2.INTER_NEAREST) for mask in tracking_seg]\n",
    "        )\n",
    "        # print(\"resized_tracking_seg\", resized_tracking_seg)\n",
    "        # Append the resized tracking_seg to the list\n",
    "        tracking_seg_masks_list.append(resized_tracking_seg)\n",
    "        # print(\"tracking_seg_masks_list\", tracking_seg_masks_list)\n",
    "        frame_count += 1\n",
    "\n",
    "    # Determine the maximum number of layers across all masks\n",
    "    max_layers = max(mask.shape[0] for mask in tracking_seg_masks_list)\n",
    "\n",
    "    # Pad all masks to have the same number of layers\n",
    "    padded_tracking_seg_masks_list = []\n",
    "    for mask in tracking_seg_masks_list:\n",
    "        # print(\"ssss\", mask.shape)\n",
    "        num_layers = mask.shape[0]\n",
    "        if num_layers < max_layers:\n",
    "            # Pad with zeros\n",
    "            padding = np.zeros((max_layers - num_layers, mask.shape[1], mask.shape[2]), dtype=mask.dtype)\n",
    "            padded_mask = np.concatenate([mask, padding], axis=0)\n",
    "        else:\n",
    "            padded_mask = mask\n",
    "        padded_tracking_seg_masks_list.append(padded_mask)\n",
    "\n",
    "    # Stack all padded masks and resized images into single arrays\n",
    "    tracking_seg_masks = np.stack(padded_tracking_seg_masks_list, axis=0)\n",
    "    resized_images = np.stack(resized_images_list, axis=0)\n",
    "\n",
    "    return tracking_seg_masks, resized_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "def resize_mask(mask: np.ndarray, new_size: tuple[int, int]) -> np.ndarray:\n",
    "    \"\"\"Resize a mask to a new resolution.\"\"\"\n",
    "    assert mask.ndim == 2, \"Mask should be 2D\"\n",
    "    return np.array(\n",
    "        Image.fromarray(mask).resize(new_size[::-1], resample=Image.Resampling.NEAREST)\n",
    "    )\n",
    "\n",
    "def resize_image(image: np.ndarray, new_size: tuple[int, int]) -> np.ndarray:\n",
    "    \"\"\"Resize an image to a new resolution.\"\"\"\n",
    "    assert image.ndim == 3, \"Image should be 3D\"\n",
    "    return np.array(\n",
    "        Image.fromarray(image).resize(new_size[::-1], resample=Image.Resampling.BICUBIC)\n",
    "    )\n",
    "\n",
    "def ensure_same_image_sizes(images: list[np.ndarray]) -> list[np.ndarray]:\n",
    "    \"\"\"Ensure all images have the same size.\"\"\"\n",
    "    sizes = Counter([img.shape[:2] for img in images])\n",
    "    if len(sizes) <= 1:\n",
    "        return images\n",
    "    \n",
    "    common_size = sizes.most_common(1)[0][0]\n",
    "    return [resize_image(img, common_size) for img in images]\n",
    "\n",
    "class ObjectTracker:\n",
    "    \"\"\"Generic object tracker that can be used with any detection model.\"\"\"\n",
    "\n",
    "    def __init__(self, iou_threshold: float = 0.3):\n",
    "        self.prev_shape = None\n",
    "        self.prev_detections = []  # List of detected objects (dicts)\n",
    "        self.id_counter = 0\n",
    "        self.iou_threshold = iou_threshold\n",
    "\n",
    "    def _compute_iou(self, mask1: np.ndarray, mask2: np.ndarray) -> float:\n",
    "        \"\"\"Compute IoU between two masks.\"\"\"\n",
    "        intersection = (mask1 & mask2).sum()\n",
    "        union = (mask1 | mask2).sum()\n",
    "        return intersection / union if union > 0 else 0\n",
    "\n",
    "    def _match_detections(self, new_detections: list[dict]) -> dict[int, dict]:\n",
    "        \"\"\"Match new detections with previous ones using IoU.\"\"\"\n",
    "        matches = {}\n",
    "        used_indices = set()\n",
    "\n",
    "        for prev in self.prev_detections:\n",
    "            best_iou, best_idx = 0, None\n",
    "            for i, new_det in enumerate(new_detections):\n",
    "                if i in used_indices:\n",
    "                    continue\n",
    "                iou = self._compute_iou(prev[\"mask\"], new_det[\"mask\"])\n",
    "                if iou > best_iou:\n",
    "                    best_iou, best_idx = iou, i\n",
    "\n",
    "            if best_idx is not None and best_iou >= self.iou_threshold:\n",
    "                matches[best_idx] = prev[\"id\"]\n",
    "                used_indices.add(best_idx)\n",
    "\n",
    "        return matches\n",
    "\n",
    "    def update(self, detections: list[dict]) -> dict[int, dict]:\n",
    "        \"\"\"Update tracker with new detections.\"\"\"\n",
    "        if len(detections) == 0:\n",
    "            return {}\n",
    "\n",
    "        new_size = detections[0][\"mask\"].shape\n",
    "        if self.prev_shape and self.prev_shape != new_size:\n",
    "            warnings.warn(\"Resolution changed, resizing previous detections.\")\n",
    "            self.prev_detections = [\n",
    "                {**d, \"mask\": resize_mask(d[\"mask\"], new_size)} for d in self.prev_detections\n",
    "            ]\n",
    "        self.prev_shape = new_size\n",
    "\n",
    "        matches = self._match_detections(detections)\n",
    "        for i, det in enumerate(detections):\n",
    "            det[\"id\"] = matches.get(i, self.id_counter)\n",
    "            if i not in matches:\n",
    "                self.id_counter += 1\n",
    "        \n",
    "        self.prev_detections = detections\n",
    "        return {d[\"id\"]: d for d in self.prev_detections}\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"Reset the tracker.\"\"\"\n",
    "        self.prev_shape = None\n",
    "        self.prev_detections = []\n",
    "        self.id_counter = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply Tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import csv\n",
    "from collections import defaultdict\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Configuration for YOLO and SAM2\n",
    "config = {\n",
    "    \"yolo_model\": \"../models/yolo_11mseg_finetuned_stage4.pt\",\n",
    "    \"sam2_cfg\": \"configs/sam2.1/sam2.1_hiera_b+.yaml\",\n",
    "    \"sam2_checkpoint\": \"../models/sam2.1l_finetuned.pt\",\n",
    "    \"yolo_threshold\": 0.5,\n",
    "}\n",
    "device = \"cuda:1\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Run inference\n",
    "model_sam2 = sam2_model(config, device)\n",
    "\n",
    "csv_headers = [\"Plant\", \"Rep\", \"Model\", \"Tracker\"]\n",
    "\n",
    "# Create results_tracking folder\n",
    "results_tracking_dir = Path(\"results_tracking\")\n",
    "results_tracking_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Ensure the 'results_csv' directory exists\n",
    "csv_dir = Path(\"results_tracking/results_csv\")\n",
    "csv_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Define the CSV file paths\n",
    "csv_file_path = csv_dir / \"tracking_results.csv\"\n",
    "total_csv_file_path = csv_dir / \"total_tracking.csv\"\n",
    "\n",
    "# Write the header once at the beginning\n",
    "with open(csv_file_path, mode='w', newline='') as file:\n",
    "    writer = csv.DictWriter(file, fieldnames=csv_headers)\n",
    "    writer.writeheader()\n",
    "    \n",
    "# Process data\n",
    "processed_data = process_plants(subselected_ds)\n",
    "\n",
    "# Write the header once at the beginning\n",
    "with open(csv_file_path, mode='w', newline='') as file:\n",
    "    writer = csv.DictWriter(file, fieldnames=csv_headers)\n",
    "    writer.writeheader()\n",
    "\n",
    "# Initialize dictionary for aggregating metrics\n",
    "aggregated_results = defaultdict(lambda: defaultdict(list))\n",
    "\n",
    "unique_color_mapping = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main processing loop\n",
    "for data in tqdm(iterate_and_return_files_with_validation(processed_data), desc=\"Processing plants\"):\n",
    "    plant_name = data['plant_name']\n",
    "    rep_name = data['rep_name']\n",
    "    image_files = data['image_files']\n",
    "    mask_files = data['mask_files']\n",
    "    # Collect results for this rep\n",
    "    rep_results = []\n",
    "\n",
    "    # Iterate over all models (YOLO and SAM2)\n",
    "    for model_name, model in tqdm(models.items(), desc=f\"Models for {rep_name}\", leave=False):\n",
    "        # Set mode based on model type\n",
    "        mode = \"sam\" if model_name == \"sam2\" else \"detectron2\" if model_name == \"detectron2\" else \"yolo\"\n",
    "        # Iterate over all tracker configurations\n",
    "        for config in tqdm(trackers_config, desc=f\"Trackers for {model_name}\", leave=False):\n",
    "            try:\n",
    "                tracker_name = config[\"name\"]\n",
    "                if tracker_name != \"botsort\":\n",
    "                    tracker_params = config[\"params\"]\n",
    "                            \n",
    "                tracker_dir = results_tracking_dir / model_name / tracker_name / plant_name / rep_name\n",
    "                sub_mask_black = tracker_dir / \"all_masks_on_black\"\n",
    "                sub_mask_image = tracker_dir / \"all_masks_on_image\"\n",
    "                \n",
    "                tracker_dir.mkdir(parents=True, exist_ok=True)\n",
    "                tracker_dir = Path(tracker_dir)\n",
    "                \n",
    "                sub_mask_black.mkdir(parents=True, exist_ok=True)\n",
    "                sub_mask_black = Path(sub_mask_black)\n",
    "                \n",
    "                sub_mask_image.mkdir(parents=True, exist_ok=True)\n",
    "                sub_mask_image = Path(sub_mask_image)\n",
    "\n",
    "                # # Initialize the tracker\n",
    "                if tracker_name != \"botsort\":\n",
    "                    tracker = initialize_tracker(tracker_name, **tracker_params)\n",
    "                else:\n",
    "                    tracker = ObjectTracker(iou_threshold=0.3)\n",
    "    \n",
    "                # Process tracking segmentation masks\n",
    "                if mode == \"detectron2\"\n",
    "                    tracking_seg_masks, original_images = process_tracking_custom_seg_masks(image_files, model, tracker, generate_unique_color, mode=mode)\n",
    "                else:\n",
    "                    tracking_seg_masks, original_images = process_tracking_seg_masks(image_files, model, tracker, generate_unique_color, mode=mode)\n",
    "\n",
    "                # print(tracking_seg_masks.shape, np.unique(tracking_seg_masks))\n",
    "                aligned_ground_truth = process_ground_truth_masks(mask_files, tracking_seg_masks)\n",
    "    \n",
    "                # Evaluate tracking performance\n",
    "                metrics = evaluate_tracking_performance_with_id_tracking(aligned_ground_truth, tracking_seg_masks, overlap_threshold=0.1)\n",
    "    \n",
    "                # Log and store results\n",
    "                print(f\"Plant: {plant_name} | Rep: {rep_name} | Model: {model_name} | Tracker: {tracker_name}\")\n",
    "                result_row = {\n",
    "                    \"Plant\": plant_name,\n",
    "                    \"Rep\": rep_name,\n",
    "                    \"Model\": model_name,\n",
    "                    \"Tracker\": tracker_name\n",
    "                }\n",
    "        \n",
    "                rep_results.append(result_row)\n",
    "    \n",
    "                for metric_name, value in metrics.items():\n",
    "                    result_row[metric_name] = value\n",
    "                    aggregated_results[(model_name, tracker_name)][metric_name].append(value)\n",
    "                        \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing tracker {tracker_name} for plant {plant_name}, rep {rep_name}: {e}\")\n",
    "\n",
    "    \n",
    "    # Dynamically update CSV headers to include new metrics\n",
    "    new_headers = set(result_row.keys()) - set(csv_headers)\n",
    "    if new_headers:\n",
    "        csv_headers.extend(new_headers)\n",
    "        with open(csv_file_path, mode='w', newline='') as file:\n",
    "            writer = csv.DictWriter(file, fieldnames=csv_headers)\n",
    "            writer.writeheader()\n",
    "            writer.writerows(rep_results)\n",
    "    else:\n",
    "        with open(csv_file_path, mode='a', newline='') as file:\n",
    "            writer = csv.DictWriter(file, fieldnames=csv_headers)\n",
    "            writer.writerows(rep_results)\n",
    "\n",
    "    print(f\"Results for rep {rep_name} saved to {csv_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "# Step 1: Read the CSV file\n",
    "df = pd.read_csv(\"results_tracking/results_csv/tracking_results.csv\")\n",
    "df_cleaned = df.drop_duplicates()\n",
    "df_cleaned.to_csv(\"results_tracking/results_csv/tracking_results_cleaned.csv\", index=False)\n",
    "print(\"Duplicate rows removed and saved as 'cleaned_output.csv'\")\n",
    "\n",
    "# Define headers for the CSV file\n",
    "average_headers = [\"Model\", \"Tracker\"] + list(next(iter(aggregated_results.values())).keys()) \n",
    "average_rows = []\n",
    "\n",
    "# Iterate over the aggregated results\n",
    "for (model_name, tracker_name), metrics in aggregated_results.items():\n",
    "    avg_row = {\n",
    "        \"Model\": model_name,\n",
    "        \"Tracker\": tracker_name\n",
    "    }\n",
    "    \n",
    "    total_id_switches = 0\n",
    "    total_fn = 0\n",
    "    total_fp = 0\n",
    "    ground_truth_count = 0\n",
    "    framebasedious = []\n",
    "    multi_object_tracking_precisions = []\n",
    "\n",
    "    for metric_name, values in metrics.items():\n",
    "        # Compute the average for metrics\n",
    "        avg_value = sum(values) / len(values)\n",
    "        \n",
    "        if metric_name.lower() not in [\"id\", \"groundtruthmaskscount\"]:\n",
    "            avg_row[metric_name] = round(avg_value, 3)\n",
    "        else:\n",
    "            avg_row[metric_name] = int(round(avg_value))\n",
    "        \n",
    "        # Collect specific metrics for additional calculations\n",
    "        if metric_name.lower() == \"idswitches\":\n",
    "            total_id_switches = sum(values)\n",
    "        elif metric_name.lower() == \"falsenegatives\":\n",
    "            total_fn = sum(values)\n",
    "        elif metric_name.lower() == \"falsepositives\":\n",
    "            total_fp = sum(values)\n",
    "        elif metric_name.lower() == \"groundtruthmaskscount\":\n",
    "            ground_truth_count = sum(values)\n",
    "        elif metric_name.lower() == \"framebasediou\":\n",
    "            framebasedious.extend(values)  # Fix: use extend\n",
    "        elif metric_name.lower() == \"multiobjecttrackingprecision\":\n",
    "            multi_object_tracking_precisions.extend(values)  # Fix: use extend\n",
    "    \n",
    "    # Compute additional metrics\n",
    "    multi_object_tracking_accuracy = (\n",
    "        1 - (total_fn + total_fp + total_id_switches) / ground_truth_count\n",
    "        if ground_truth_count > 0 else 0.0\n",
    "    )\n",
    "    multi_object_tracking_precision = (\n",
    "        sum(multi_object_tracking_precisions) / len(multi_object_tracking_precisions) if multi_object_tracking_precisions else 0.0\n",
    "    )\n",
    "    frame_based_iou = (\n",
    "        sum(framebasedious) / len(framebasedious) if framebasedious else 0.0\n",
    "    )\n",
    "    \n",
    "    # Add computed values to the row\n",
    "    avg_row[\"GroundTruthMasksCount\"] = round(ground_truth_count, 3)\n",
    "    avg_row[\"IDSwitches\"] = round(total_id_switches, 3)\n",
    "    avg_row[\"MultiObjectTrackingAccuracy\"] = round(multi_object_tracking_accuracy, 3)\n",
    "    avg_row[\"MultiObjectTrackingPrecision\"] = round(multi_object_tracking_precision, 3)\n",
    "    avg_row[\"FalseNegatives\"] = round(total_fn, 3)\n",
    "    avg_row[\"FalsePositives\"] = round(total_fp, 3)\n",
    "    avg_row[\"FrameBasedIOU\"]  = round(frame_based_iou, 3)\n",
    "    \n",
    "    average_rows.append(avg_row)\n",
    "\n",
    "# Write the results to the CSV file\n",
    "with open(total_csv_file_path, mode='w', newline='') as file:\n",
    "    writer = csv.DictWriter(file, fieldnames=average_headers)\n",
    "    writer.writeheader()\n",
    "    writer.writerows(average_rows)\n",
    "\n",
    "print(f\"Averages saved to {total_csv_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from collections import defaultdict\n",
    "\n",
    "total_csv_file_path = \"results_tracking/results_csv/tracking_results_cleaned.csv\"\n",
    "\n",
    "# Read the CSV file and aggregate results\n",
    "aggregated_results = defaultdict(lambda: defaultdict(list))\n",
    "with open(total_csv_file_path, mode='r', newline='') as file:\n",
    "    reader = csv.DictReader(file)\n",
    "    for row in reader:\n",
    "        model_name = row[\"Model\"]\n",
    "        tracker_name = row[\"Tracker\"]\n",
    "        \n",
    "        for metric_name, value in row.items():\n",
    "            if metric_name in [\"Model\", \"Tracker\"]:\n",
    "                continue\n",
    "            try:\n",
    "                aggregated_results[(model_name, tracker_name)][metric_name].append(float(value))\n",
    "            except ValueError:\n",
    "                pass  # Ignore non-numeric values\n",
    "\n",
    "# Define headers for the output CSV file in the exact order\n",
    "average_headers = [\n",
    "    \"Model\",\n",
    "    \"Tracker\",\n",
    "    \"GroundTruthMasksCount\",\n",
    "    \"MultiObjectTrackingAccuracy\",\n",
    "    \"IDSwitches\",\n",
    "    \"MultiObjectTrackingPrecision\",\n",
    "    \"FalseNegatives\",\n",
    "    \"FalsePositives\",\n",
    "    \"FrameBasedIOU\"\n",
    "]\n",
    "average_rows = []\n",
    "\n",
    "# Process aggregated results\n",
    "for (model_name, tracker_name), metrics in aggregated_results.items():\n",
    "    avg_row = {\n",
    "        \"Model\": model_name,\n",
    "        \"Tracker\": tracker_name\n",
    "    }\n",
    "    \n",
    "    total_id_switches = 0\n",
    "    total_fn = 0\n",
    "    total_fp = 0\n",
    "    ground_truth_count = 0\n",
    "    framebasedious = []\n",
    "    multi_object_tracking_precisions = []\n",
    "\n",
    "    for metric_name, values in metrics.items():\n",
    "        avg_value = sum(values) / len(values) if values else 0\n",
    "        \n",
    "        if metric_name.lower() == \"idswitches\":\n",
    "            total_id_switches = sum(values)\n",
    "        elif metric_name.lower() == \"falsenegatives\":\n",
    "            total_fn = sum(values)\n",
    "        elif metric_name.lower() == \"falsepositives\":\n",
    "            total_fp = sum(values)\n",
    "        elif metric_name.lower() == \"groundtruthmaskscount\":\n",
    "            ground_truth_count = sum(values)\n",
    "        elif metric_name.lower() == \"framebasediou\":\n",
    "            framebasedious.extend(values)  \n",
    "        elif metric_name.lower() == \"multiobjecttrackingprecision\":\n",
    "            multi_object_tracking_precisions.extend(values)  \n",
    "    \n",
    "    # Compute additional metrics\n",
    "    multi_object_tracking_accuracy = (\n",
    "        1 - (total_fn + total_fp + total_id_switches) / ground_truth_count\n",
    "        if ground_truth_count > 0 else 0.0\n",
    "    )\n",
    "    multi_object_tracking_precision = (\n",
    "        sum(multi_object_tracking_precisions) / len(multi_object_tracking_precisions) if multi_object_tracking_precisions else 0.0\n",
    "    )\n",
    "    frame_based_iou = (\n",
    "        sum(framebasedious) / len(framebasedious) if framebasedious else 0.0\n",
    "    )\n",
    "    \n",
    "    # Add computed values to the row\n",
    "    avg_row[\"GroundTruthMasksCount\"] = round(ground_truth_count, 3)\n",
    "    avg_row[\"MultiObjectTrackingAccuracy\"] = round(multi_object_tracking_accuracy, 3)\n",
    "    avg_row[\"IDSwitches\"] = round(total_id_switches, 3)\n",
    "    avg_row[\"MultiObjectTrackingPrecision\"] = round(multi_object_tracking_precision, 3)\n",
    "    avg_row[\"FalseNegatives\"] = round(total_fn, 3)\n",
    "    avg_row[\"FalsePositives\"] = round(total_fp, 3)\n",
    "    avg_row[\"FrameBasedIOU\"]  = round(frame_based_iou, 3)\n",
    "    \n",
    "    # Append only the specified fields in the desired order\n",
    "    average_rows.append({key: avg_row.get(key, \"\") for key in average_headers})\n",
    "\n",
    "# Write the results to the output CSV file\n",
    "output_csv_file_path = \"results_tracking/results_csv/total_tracking_results.csv\"\n",
    "\n",
    "with open(output_csv_file_path, mode='w', newline='') as file:\n",
    "    writer = csv.DictWriter(file, fieldnames=average_headers)\n",
    "    writer.writeheader()\n",
    "    writer.writerows(average_rows)\n",
    "\n",
    "print(f\"Averages saved to {output_csv_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# Load the CSV file\n",
    "total_csv_file_path = \"results_tracking/results_csv/total_tracking.csv\"\n",
    "data = pd.read_csv(total_csv_file_path)\n",
    "# Find the best tracker for each model based on MultiObjectTrackingAccuracy\n",
    "best_trackers = data.loc[data.groupby(\"Model\")[\"MultiObjectTrackingAccuracy\"].idxmax()]\n",
    "# Save the best trackers to a new CSV file or print them\n",
    "best_trackers_csv_path = \"results_tracking/results_csv/best_trackers.csv\"\n",
    "best_trackers.to_csv(best_trackers_csv_path, index=False)\n",
    "# Load the best models CSV\n",
    "best_models_csv_path = \"results_tracking/results_csv/best_trackers.csv\"\n",
    "best_models_df = pd.read_csv(best_models_csv_path)\n",
    "# Create a dictionary mapping each model to its best tracker\n",
    "best_tracker_per_model = {row['Model']: row['Tracker'] for _, row in best_models_df.iterrows()}\n",
    "print(\"Best trackers for each model:\")\n",
    "print(best_trackers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Select Best Combination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration for YOLO and SAM2\n",
    "config = {\n",
    "    \"yolo_model\": \"../models/yolo_11mseg_finetuned_stage4.pt\",\n",
    "    \"sam2_cfg\": \"configs/sam2.1/sam2.1_hiera_b+.yaml\",\n",
    "    \"sam2_checkpoint\": \"../models/sam2.1l_finetuned.pt\",\n",
    "    \"yolo_threshold\": 0.5,\n",
    "}\n",
    "device = \"cuda:1\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Run inference\n",
    "model_sam2 = sam2_model(config, device)\n",
    "\n",
    "csv_headers = [\"Plant\", \"Rep\", \"Model\", \"Tracker\"]\n",
    "\n",
    "# Create results_tracking folder\n",
    "results_tracking_dir = Path(\"results_tracking\")\n",
    "results_tracking_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Ensure the 'results_csv' directory exists\n",
    "csv_dir = Path(\"results_tracking/results_csv\")\n",
    "csv_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Define the CSV file paths\n",
    "csv_file_path = csv_dir / \"tracking_results.csv\"\n",
    "total_csv_file_path = csv_dir / \"total_tracking.csv\"\n",
    "\n",
    "# Write the header once at the beginning\n",
    "with open(csv_file_path, mode='w', newline='') as file:\n",
    "    writer = csv.DictWriter(file, fieldnames=csv_headers)\n",
    "    writer.writeheader()\n",
    "    \n",
    "# Process data\n",
    "processed_data = process_plants(subselected_ds)\n",
    "\n",
    "# Write the header once at the beginning\n",
    "with open(csv_file_path, mode='w', newline='') as file:\n",
    "    writer = csv.DictWriter(file, fieldnames=csv_headers)\n",
    "    writer.writeheader()\n",
    "\n",
    "# Initialize dictionary for aggregating metrics\n",
    "aggregated_results = defaultdict(lambda: defaultdict(list))\n",
    "\n",
    "unique_color_mapping = {}\n",
    "\n",
    "# Main processing loop\n",
    "for data in tqdm(iterate_and_return_files_with_validation(processed_data), desc=\"Processing plants\"):\n",
    "    plant_name = data['plant_name']\n",
    "    rep_name = data['rep_name']\n",
    "    image_files = data['image_files']\n",
    "    mask_files = data['mask_files']\n",
    "    # Collect results for this rep\n",
    "    rep_results = []\n",
    "\n",
    "    # Iterate over all models (YOLO and SAM2)\n",
    "    for model_name, model in tqdm(models.items(), desc=f\"Models for {rep_name}\", leave=False):\n",
    "        print(\"model_name\",model_name)\n",
    "        # Set mode based on model type\n",
    "        mode = \"sam\" if model_name == \"sam2\" else \"detectron2\" if model_name == \"detectron2\" else \"yolo\"\n",
    "        # Iterate over all tracker configurations\n",
    "        for config in tqdm(trackers_config, desc=f\"Trackers for {model_name}\", leave=False):\n",
    "\n",
    "            tracker_name = config[\"name\"]\n",
    "            if tracker_name != \"botsort\":\n",
    "                tracker_params = config[\"params\"]\n",
    "                  \n",
    "            if best_tracker_per_model[model_name] != tracker_name:\n",
    "                continue\n",
    "            tracker_dir = results_tracking_dir / model_name / tracker_name / plant_name / rep_name\n",
    "            sub_mask_black = tracker_dir / \"all_masks_on_black\"\n",
    "            sub_mask_image = tracker_dir / \"all_masks_on_image\"\n",
    "            \n",
    "            tracker_dir.mkdir(parents=True, exist_ok=True)\n",
    "            tracker_dir = Path(tracker_dir)\n",
    "            \n",
    "            sub_mask_black.mkdir(parents=True, exist_ok=True)\n",
    "            sub_mask_black = Path(sub_mask_black)\n",
    "            \n",
    "            sub_mask_image.mkdir(parents=True, exist_ok=True)\n",
    "            sub_mask_image = Path(sub_mask_image)\n",
    "            \n",
    "            # # Initialize the tracker\n",
    "            if tracker_name != \"botsort\":\n",
    "                tracker = initialize_tracker(tracker_name, **tracker_params)\n",
    "            else:\n",
    "                tracker = ObjectTracker(iou_threshold=0.3)\n",
    "\n",
    "            # Process tracking segmentation masks\n",
    "            if mode == \"detectron2\"\n",
    "                tracking_seg_masks, original_images = process_tracking_custom_seg_masks(image_files, model, tracker, generate_unique_color, mode=mode)\n",
    "            else:\n",
    "                tracking_seg_masks, original_images = process_tracking_seg_masks(image_files, model, tracker, generate_unique_color, mode=mode)\n",
    "\n",
    "            # # print(tracking_seg_masks.shape, np.unique(tracking_seg_masks))\n",
    "            aligned_ground_truth = process_ground_truth_masks(mask_files, tracking_seg_masks)\n",
    "            \n",
    "            # Evaluate tracking performance\n",
    "            metrics = evaluate_tracking_performance_with_id_tracking(aligned_ground_truth, tracking_seg_masks, overlap_threshold=0.1)\n",
    "            \n",
    "            # Log and store results\n",
    "            print(f\"Plant: {plant_name} | Rep: {rep_name} | Model: {model_name} | Tracker: {tracker_name}\")\n",
    "            result_row = {\n",
    "                \"Plant\": plant_name,\n",
    "                \"Rep\": rep_name,\n",
    "                \"Model\": model_name,\n",
    "                \"Tracker\": tracker_name\n",
    "            }\n",
    "            \n",
    "            rep_results.append(result_row)\n",
    "            \n",
    "            for metric_name, value in metrics.items():\n",
    "                result_row[metric_name] = value\n",
    "                aggregated_results[(model_name, tracker_name)][metric_name].append(value)\n",
    "                    \n",
    "            save_instance_masks(tracking_seg_masks, original_images, tracker_dir)\n",
    "            # Process each frame and save visualizations\n",
    "            for frame_index in range(tracking_seg_masks.shape[0]):\n",
    "                instance_layers = tracking_seg_masks[frame_index]\n",
    "                original_image = original_images[frame_index]\n",
    "                max_mask = instance_layers.max(axis=0)\n",
    "\n",
    "                # Save visualization for the current frame\n",
    "                image_mask_path = sub_mask_image / f\"frame_{frame_index:04d}.png\"\n",
    "                black_mask_path = sub_mask_black / f\"frame_{frame_index:04d}.png\"\n",
    "\n",
    "                # Generate unique colors for each instance ID and remap IDs to start from 1, 2, 3, ...\n",
    "                unique_id_mapping = {old_id: new_id for new_id, old_id in enumerate(sorted(np.unique(max_mask)), start=1) if old_id != 0}\n",
    "                remapped_mask = np.zeros_like(max_mask)\n",
    "                \n",
    "                for old_id, new_id in unique_id_mapping.items():\n",
    "                    remapped_mask[max_mask == old_id] = new_id\n",
    "                \n",
    "                # Generate unique colors for remapped IDs\n",
    "                unique_colors = {instance_id: generate_unique_color(instance_id) for instance_id in unique_id_mapping.values()}\n",
    "\n",
    "                save_visualization(image_mask_path, black_mask_path, frame_index, remapped_mask, unique_colors, original_image)\n",
    "\n",
    "                rep_results.append(result_row)\n",
    "    \n",
    "\n",
    "    # Dynamically update CSV headers to include new metrics\n",
    "    new_headers = set(result_row.keys()) - set(csv_headers)\n",
    "    if new_headers:\n",
    "        csv_headers.extend(new_headers)\n",
    "        with open(csv_file_path, mode='w', newline='') as file:\n",
    "            writer = csv.DictWriter(file, fieldnames=csv_headers)\n",
    "            writer.writeheader()\n",
    "            writer.writerows(rep_results)\n",
    "    else:\n",
    "        with open(csv_file_path, mode='a', newline='') as file:\n",
    "            writer = csv.DictWriter(file, fieldnames=csv_headers)\n",
    "            writer.writerows(rep_results)\n",
    "\n",
    "    print(f\"Results for rep {rep_name} saved to {csv_file_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
