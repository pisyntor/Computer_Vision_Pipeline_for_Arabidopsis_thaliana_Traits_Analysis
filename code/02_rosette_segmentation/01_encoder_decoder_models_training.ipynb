{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3H6-g17F-KyY",
    "outputId": "aa51c2c8-4c2b-4088-a7e8-461e08db4fd9"
   },
   "outputs": [],
   "source": [
    "# General utilities\n",
    "import os\n",
    "import gc\n",
    "import re\n",
    "import json\n",
    "import time\n",
    "import shutil\n",
    "import pickle\n",
    "\n",
    "# Numerical and data handling\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Image processing\n",
    "import cv2\n",
    "\n",
    "# Visualization\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# PyTorch and related libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim.lr_scheduler import _LRScheduler\n",
    "\n",
    "# Torchvision\n",
    "import torchvision\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as T\n",
    "\n",
    "# Segmentation models\n",
    "import segmentation_models_pytorch as smp\n",
    "from segmentation_models_pytorch.losses import DiceLoss\n",
    "\n",
    "# Evaluation\n",
    "from torchmetrics import ConfusionMatrix\n",
    "\n",
    "# Progress bar\n",
    "from tqdm import tqdm\n",
    "\n",
    "default_matplotlib_backend = matplotlib.get_backend()\n",
    "print('imported')\n",
    "print('default_matplotlib_backend: {}'.format(default_matplotlib_backend))\n",
    "print(' numpy version', np.__version__) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "N0skx1mP-K2T",
    "outputId": "27a4efc5-8476-40dd-d6ba-23f92d708355"
   },
   "outputs": [],
   "source": [
    "# Important, to have the same repartition of data between different machines\n",
    "np.random.seed(42)\n",
    "RUN_MODE = \"RUN\"\n",
    "root_path = '' # path of the root project folder\n",
    "weights_path =  ''\n",
    "stats_path = ''\n",
    "\n",
    "USE_2_GPUS = False\n",
    "\n",
    "if USE_2_GPUS:\n",
    "    # working device\n",
    "    os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"  # Arrange GPU devices starting from 0\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"]= \"0,1\" \n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print('Selected device: {}'.format(device))\n",
    "else:\n",
    "    # working device\n",
    "    device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print('Selected device: {}'.format(device))\n",
    "\n",
    "if device == 'cuda':\n",
    "        print('Device name: {}'.format(torch.cuda.get_device_name(device)))\n",
    "\n",
    "encoders  = ['resnet34', 'timm-regnetx_160', 'timm-regnety_160', 'mobilenet_v2']\n",
    "\n",
    "encoders_v2 = ['resnet34', 'resnext50_32x4d', 'resnet101', 'resnet152',\n",
    "            'timm-regnetx_064', 'timm-regnety_064', 'resnext101_32x8d',\n",
    "            'se_resnet50', 'se_resnet152', 'se_resnext101_32x4d']\n",
    "decoders = ['unet', 'unet++', 'pspnet', 'deeplabv3+']\n",
    "#decoders_v2 = ['unet', 'unet++', 'pspnet', 'deeplabv3+']\n",
    "#encoders = ['timm-resnest26d', 'resnext50_32x4d']\n",
    "decoders_v2 = ['deeplabv3+']\n",
    "\n",
    "print(f\"number of encoders {len(encoders) + len(encoders_v2)}\")\n",
    "print(f\"number of decoders {len(decoders) + len(decoders_v2)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Number of GPUS available: \", torch.cuda.device_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EOF0cGnOxmjK",
    "outputId": "a8a7b3e8-246e-4445-b879-1b1f58aec95f"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import torchvision.transforms.functional as TF\n",
    "import random\n",
    "import imgaug.augmenters as iaa\n",
    "from imgaug import SegmentationMapsOnImage\n",
    "\n",
    "train_valid_test_split_json_name = ''\n",
    "with open(os.path.join(root_path, train_valid_test_split_json_name), 'r') as f:\n",
    "    data_dict = json.load(f)\n",
    "\n",
    "\n",
    "augmentations_list = [\n",
    "                    iaa.Sharpen(alpha=(0, 0.5), lightness=(2.5, 3.0)),\n",
    "                    iaa.GammaContrast((0.4, 0.9)),  # previous values iaa.GammaContrast((0.4, 0.9))\n",
    "                    iaa.AddToHueAndSaturation((-40, 20)),\n",
    "                    iaa.Multiply((1.1, 1.8))   # previous values iaa.Multiply((0.9, 1.1))\n",
    "                    \n",
    "                ]\n",
    "zoomin_list = [\n",
    "    iaa.Affine(scale=(1.2, 1.8)), \n",
    "    iaa.Crop(percent=(0.01, 0.3))\n",
    "]\n",
    "\n",
    "bg_images_folder = \"\"\n",
    "\n",
    "\n",
    "# Custom Dataset class to load images and masks\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, image_paths, mask_paths, aug=None):\n",
    "        self.image_paths = image_paths\n",
    "        self.mask_paths = mask_paths\n",
    "        self.aug = aug\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def transform(self, image, mask):\n",
    "        # Resize\n",
    "        resize = transforms.Resize(size=(512, 512))\n",
    "        \n",
    "\n",
    "        if self.aug:\n",
    "            if random.random() > 0.5:\n",
    "            # Random horizontal flipping\n",
    "                image = TF.hflip(image)\n",
    "                mask = TF.hflip(mask)\n",
    "\n",
    "            # Random vertical flipping\n",
    "            if random.random() > 0.5:\n",
    "                image = TF.vflip(image)\n",
    "                mask = TF.vflip(mask)\n",
    "\n",
    "            # Random rotating\n",
    "            if random.random() > 0.5:\n",
    "                angle = random.randrange(45, 270)\n",
    "                image = TF.rotate(image, angle)\n",
    "                mask = TF.rotate(mask, angle)\n",
    "\n",
    "            if random.random() > 0.6:\n",
    "                image_np = np.array(image)\n",
    "                bg_images_folder = \"\"  # Update with your BG folder path\n",
    "                bg_images_list = [\n",
    "                    f for f in os.listdir(bg_images_folder)\n",
    "                    if f.lower().endswith(('.png', '.jpg', '.jpeg'))\n",
    "                ]\n",
    "\n",
    "                if not bg_images_list:\n",
    "                    raise ValueError(\"No valid background images found in the specified folder.\")\n",
    "\n",
    "                bg_image_choice = random.choice(bg_images_list)\n",
    "                bg_image_path = os.path.join(bg_images_folder, bg_image_choice)\n",
    "\n",
    "                # Read the background image\n",
    "                background_image = cv2.imread(bg_image_path)\n",
    "\n",
    "                if background_image is None:\n",
    "                    raise ValueError(f\"Failed to load background image: {bg_image_path}\")\n",
    "\n",
    "                # Resize the background image\n",
    "                background_image = cv2.resize(background_image, (image_np.shape[1], image_np.shape[0]))\n",
    "\n",
    "                # Combine with segmentation\n",
    "                seg_image = np.where(mask == 0, 0, image_np)\n",
    "                seg_image_with_background = np.where(seg_image != 0, 0, background_image)\n",
    "                image = cv2.add(seg_image, seg_image_with_background)\n",
    "                image = Image.fromarray(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
    "\n",
    "            if random.random() > 0.5:\n",
    "                image_np = np.array(image)\n",
    "                selected_augmentation = random.choice(augmentations_list)\n",
    "                image_np = selected_augmentation.augment_image(image_np)\n",
    "                image = Image.fromarray(image_np)\n",
    "\n",
    "            if random.random() > 0.5:\n",
    "                image_np = np.array(image)\n",
    "                mask_np = np.array(mask)\n",
    "                \n",
    "                selected_augmentation = random.choice(zoomin_list)\n",
    "                aug = iaa.Sequential([selected_augmentation])\n",
    "                mask_segmaps = SegmentationMapsOnImage(mask_np, shape=mask_np.shape)\n",
    "                zoom_image_np, masks_aug = aug(image=image_np, segmentation_maps=mask_segmaps)\n",
    "                zoom_mask_np = masks_aug.get_arr()\n",
    "                \n",
    "                image = Image.fromarray(zoom_image_np)\n",
    "                mask = Image.fromarray(zoom_mask_np)\n",
    "\n",
    "        \n",
    "        image = resize(image)\n",
    "        mask = resize(mask)\n",
    "        \n",
    "        # Transform to tensor\n",
    "        image = TF.to_tensor(image)\n",
    "        mean = [0.485, 0.456, 0.406]\n",
    "        std = [0.229, 0.224, 0.225]\n",
    "        image = TF.normalize(image, mean=mean, std=std)\n",
    "        mask = TF.to_tensor(mask)\n",
    "        return image, mask\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        image = Image.open(self.image_paths[idx])\n",
    "        mask = Image.open(self.mask_paths[idx]).convert('L')\n",
    "        image, mask = self.transform(image, mask)\n",
    "        return image, mask\n",
    "\n",
    "\n",
    "# Base directory where your images and masks are stored\n",
    "base_dir = \"\"\n",
    "base_dir_label = \"\"\n",
    "\n",
    "# Function to gather file paths based on the dictionary\n",
    "def get_file_paths(data_dict, base_dir, base_dir_label):\n",
    "    train_image_paths, val_image_paths, test_image_paths = [], [], []\n",
    "    train_mask_paths, val_mask_paths, test_mask_paths = [], [], []\n",
    "\n",
    "    for dataset, splits in data_dict.items():\n",
    "        for rep, split in splits.items():\n",
    "            for f in os.listdir(os.path.join(base_dir, dataset, rep)):\n",
    "                image_path = os.path.join(base_dir, dataset, rep, f)\n",
    "                mask_path = os.path.join(base_dir_label, dataset, rep, \"masks\", f.replace('.png', '_mask.png'))\n",
    "\n",
    "                if split == 'train':\n",
    "                    train_image_paths.append(image_path)\n",
    "                    train_mask_paths.append(mask_path)\n",
    "                elif split == 'valid':\n",
    "                    val_image_paths.append(image_path)\n",
    "                    val_mask_paths.append(mask_path)\n",
    "                elif split == 'test':\n",
    "                    test_image_paths.append(image_path)\n",
    "                    test_mask_paths.append(mask_path)\n",
    "\n",
    "    return train_image_paths, train_mask_paths, val_image_paths, val_mask_paths, test_image_paths, test_mask_paths\n",
    "\n",
    "print('Loading data ...')\n",
    "train_image_paths, train_mask_paths, val_image_paths, val_mask_paths, test_image_paths, test_mask_paths = get_file_paths(data_dict, base_dir, base_dir_label)\n",
    "\n",
    "\n",
    "print('Number of training images: {}'.format(len(train_image_paths)))\n",
    "print('Number of validation images: {}'.format(len(val_image_paths)))\n",
    "print('Number of test images: {}'.format(len(test_image_paths)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zDQ44ivl4dzE"
   },
   "outputs": [],
   "source": [
    "# Define the mean and std used for normalization (example values)\n",
    "mean = [0.485, 0.456, 0.406]\n",
    "std = [0.229, 0.224, 0.225]\n",
    "\n",
    "def unnormalize(image, mean, std):\n",
    "    \"\"\"\n",
    "    Unnormalize the image.\n",
    "\n",
    "    Args:\n",
    "    - image (torch tensor): The normalized image tensor.\n",
    "\n",
    "    Returns:\n",
    "    - unnormalized_image (numpy array): The unnormalized image.\n",
    "    \"\"\"\n",
    "    for t, m, s in zip(image, mean, std):\n",
    "        t.mul_(s).add_(m)  # Unnormalize\n",
    "    return image\n",
    "\n",
    "def plot_image_and_mask(image, mask):\n",
    "    \"\"\"\n",
    "    Plots the given image and mask side by side.\n",
    "\n",
    "    Args:\n",
    "    - image (torch tensor): The input image.\n",
    "    - mask (torch tensor): The corresponding mask.\n",
    "    \"\"\"\n",
    "    if torch.is_tensor(image):\n",
    "        image = unnormalize(image, mean, std)\n",
    "        image = image.permute(1, 2, 0).numpy()  # Convert from CxHxW to HxWxC\n",
    "        # image = image[...,::-1]\n",
    "        image = np.clip(image, 0, 1)  # Ensure values are in the range [0, 1]\n",
    "    if torch.is_tensor(mask):\n",
    "        mask = mask.permute(1, 2, 0).numpy()\n",
    "\n",
    "    plt.figure(figsize=(10, 5))\n",
    "\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.imshow(image)\n",
    "    plt.title('Image')\n",
    "    plt.axis('off')\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.imshow(mask, cmap='gray')\n",
    "    plt.title('Mask')\n",
    "    plt.axis('off')\n",
    "\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "V_JeZuaX1PuJ",
    "outputId": "5b69b01b-83e4-457b-aa22-a3dd8fc2f187"
   },
   "outputs": [],
   "source": [
    "# Create datasets\n",
    "AUGMENTATION = False\n",
    "train_dataset = CustomDataset(train_image_paths, train_mask_paths, aug=AUGMENTATION)\n",
    "val_dataset = CustomDataset(val_image_paths, val_mask_paths, aug=False)\n",
    "test_dataset = CustomDataset(test_image_paths, test_mask_paths, aug=False)\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, drop_last=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False, drop_last=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "print('Data loaders created')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TxKuuukRC9f3"
   },
   "source": [
    "### Data sample (Run this only if you want to see an example of the data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 550
    },
    "id": "c-cdl9Rq8fYL",
    "outputId": "7ab0b9aa-5e41-42ae-9d28-35b3a0c63d91"
   },
   "outputs": [],
   "source": [
    "# Example usage\n",
    "# Assuming you have a DataLoader named train_loader\n",
    "data_iter = iter(train_loader)\n",
    "images, masks = next(data_iter)\n",
    "\n",
    "# Plot the first image and mask from the batch\n",
    "plot_image_and_mask(images[0], masks[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6gGak8ErYs48"
   },
   "source": [
    "### LR Scheduler and utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-15T14:26:31.255445094Z",
     "start_time": "2023-05-15T14:26:31.231209865Z"
    },
    "id": "2Dmhcs-AYtFY"
   },
   "outputs": [],
   "source": [
    "class PolyScheduler(_LRScheduler):\n",
    "    def __init__(self, optimizer, base_lr, max_steps, warmup_steps, last_epoch=-1):\n",
    "        self.base_lr = base_lr\n",
    "        self.warmup_lr_init = 0.0001\n",
    "        self.max_steps: int = max_steps\n",
    "        self.warmup_steps: int = warmup_steps\n",
    "        self.power = 2\n",
    "        super(PolyScheduler, self).__init__(optimizer, -1, False)\n",
    "        self.last_epoch = last_epoch\n",
    "\n",
    "    def get_warmup_lr(self):\n",
    "        alpha = float(self.last_epoch) / float(self.warmup_steps)\n",
    "        return [self.base_lr * alpha for _ in self.optimizer.param_groups]\n",
    "\n",
    "    def get_lr(self):\n",
    "        if self.last_epoch == -1:\n",
    "            return [self.warmup_lr_init for _ in self.optimizer.param_groups]\n",
    "        if self.last_epoch < self.warmup_steps:\n",
    "            return self.get_warmup_lr()\n",
    "        else:\n",
    "            alpha = pow(\n",
    "                1\n",
    "                - float(self.last_epoch - self.warmup_steps)\n",
    "                / float(self.max_steps - self.warmup_steps),\n",
    "                self.power,\n",
    "            )\n",
    "            return [self.base_lr * alpha for _ in self.optimizer.param_groups]\n",
    "\n",
    "def plot_model_stats(model_name, train_loss, train_acc, train_iou, test_loss, test_acc, test_iou):\n",
    "    fig = plt.figure(figsize=(16, 5))\n",
    "    ax1 = plt.subplot(1, 3, 1)\n",
    "    plt.plot(np.arange(len(train_loss)), train_loss, label = 'Train loss')\n",
    "    plt.plot(np.arange(len(test_loss)), test_loss, label = 'Val loss')\n",
    "    plt.title('Model: {} - Validation loss: {:.4f}'.format(model_name, test_loss[-1]))\n",
    "    ax1.set_ylabel(\"Loss\")\n",
    "    ax1.set_xlabel(\"Epochs\")\n",
    "    ax1.legend(loc='upper right')\n",
    "\n",
    "    ax2 = plt.subplot(1, 3, 2)\n",
    "    plt.plot(np.arange(len(train_acc)), np.array(train_acc) * 100, color='green', label = 'Train accuracy')\n",
    "    plt.plot(np.arange(len(test_acc)), np.array(test_acc) * 100, color='red', label = 'Val accuracy')\n",
    "    plt.title('Model: {} - Validation accuracy: {:.2f} %'.format(model_name, test_acc[-1] * 100))\n",
    "    ax2.set_ylabel(\"Accuracy\")\n",
    "    ax2.set_xlabel(\"Epochs\")\n",
    "    ax2.legend(loc='lower right')\n",
    "\n",
    "    ax2 = plt.subplot(1, 3, 2)\n",
    "    plt.plot(np.arange(len(train_iou)), np.array(train_iou) * 100, color='green', label = 'Train accuracy')\n",
    "    plt.plot(np.arange(len(test_iou)), np.array(test_iou) * 100, color='red', label = 'Val accuracy')\n",
    "    plt.title('Model: {} - Validation accuracy: {:.2f} %'.format(model_name, test_acc[-1] * 100))\n",
    "    ax2.set_ylabel(\"IOU\")\n",
    "    ax2.set_xlabel(\"Epochs\")\n",
    "    ax2.legend(loc='lower right')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "def load_model_weights(BACKBONE, OPTIMIZER):\n",
    "    # this function loads the whole model with weights\n",
    "    pth = os.path.join(weights_path, 'backbone_{}_{}.pth'.format(BACKBONE, OPTIMIZER))\n",
    "    assert os.path.exists(pth), 'Configuration not found'\n",
    "\n",
    "    model = torch.load(pth).to(device)\n",
    "    print('Backbone weights loaded: {}'.format(BACKBONE))\n",
    "\n",
    "    return model\n",
    "\n",
    "def load_model_stats(BACKBONE, OPTIMIZER):\n",
    "    # this function loads training stats (train/val loss and acc)\n",
    "    stats_file = os.path.join(stats_path, 'stats_{}_{}.pkl'.format(BACKBONE, OPTIMIZER))\n",
    "    assert os.path.exists(stats_file), 'Configuration not found'\n",
    "\n",
    "    with open(stats_file, 'rb') as stats:\n",
    "        stats = pickle.load(stats)\n",
    "    print('Train/Validation stats loaded for the model: {}'.format(BACKBONE))\n",
    "\n",
    "    return stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u-jWW_2O_Yoy"
   },
   "source": [
    "## Training and evaluation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HXc33z63sDFn"
   },
   "outputs": [],
   "source": [
    "from pytorch_utils import training_utils as pt_train\n",
    "\n",
    "# Dice Loss\n",
    "dice_loss = DiceLoss(mode='binary')\n",
    "SMOOTH = 1e-6\n",
    "# Helper functions for evaluation and accuracy calculation\n",
    "def _evaluate(model: torch.nn.Module, val_loader: DataLoader, device=device):\n",
    "    model.eval()\n",
    "    outputs = []\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            batch = [tensor.to(device).float() for tensor in batch]\n",
    "            if USE_2_GPUS:\n",
    "                outputs.append(model.module.validation_step(batch, model))\n",
    "            else:\n",
    "                outputs.append(model.validation_step(batch, model))\n",
    "    if USE_2_GPUS:\n",
    "        return model.module.validation_epoch_end(outputs)\n",
    "    else:\n",
    "        return model.validation_epoch_end(outputs)\n",
    "\n",
    "def _accuracy(outputs: torch.Tensor, labels: torch.Tensor):\n",
    "    prob_mask = outputs.sigmoid()\n",
    "    pred_mask = (prob_mask > 0.5).float()\n",
    "    tp, fp, fn, tn = smp.metrics.get_stats(pred_mask.long(), labels.long(), mode=\"binary\")\n",
    "    return smp.metrics.accuracy(tp, fp, fn, tn, reduction=\"macro\")\n",
    "\n",
    "def _iou(outputs: torch.Tensor, labels: torch.Tensor):\n",
    "    prob_mask = outputs.sigmoid()\n",
    "    pred_mask = (prob_mask > 0.5).float()\n",
    "    tp, fp, fn, tn = smp.metrics.get_stats(pred_mask.long(), labels.long(), mode=\"binary\")\n",
    "    return smp.metrics.iou_score(tp, fp, fn, tn, reduction=\"micro\")\n",
    "\n",
    "def _iou_plant(outputs: torch.Tensor, labels: torch.Tensor):\n",
    "    prob_mask = outputs.sigmoid()\n",
    "    pred_mask = (prob_mask > 0.5).float()\n",
    "    tp, fp, fn, tn = smp.metrics.get_stats(pred_mask.long(), labels.long(), mode=\"binary\")\n",
    "    iou_class_1 = smp.metrics.iou_score(tp[1], fp[1], fn[1], tn[1], reduction=\"micro\")\n",
    "    return iou_class_1\n",
    "\n",
    "def precision_score(outputs: torch.Tensor, labels: torch.Tensor):\n",
    "    prob_mask = outputs.sigmoid()\n",
    "    pred_mask = (prob_mask > 0.5).float()\n",
    "    tp, fp, fn, tn = smp.metrics.get_stats(pred_mask.long(), labels.long(), mode=\"binary\")\n",
    "    return smp.metrics.positive_predictive_value(tp, fp, fn, tn, reduction=\"micro\")\n",
    "\n",
    "def recall_score(outputs: torch.Tensor, labels: torch.Tensor):\n",
    "    prob_mask = outputs.sigmoid()\n",
    "    pred_mask = (prob_mask > 0.5).float()\n",
    "    tp, fp, fn, tn = smp.metrics.get_stats(pred_mask.long(), labels.long(), mode=\"binary\")\n",
    "    return smp.metrics.recall(tp, fp, fn, tn, reduction=\"micro-imagewise\")\n",
    "\n",
    "def f1_score(outputs: torch.Tensor, labels: torch.Tensor):\n",
    "    prob_mask = outputs.sigmoid()\n",
    "    pred_mask = (prob_mask > 0.5).float()\n",
    "    tp, fp, fn, tn = smp.metrics.get_stats(pred_mask.long(), labels.long(), mode=\"binary\")\n",
    "    return smp.metrics.f1_score(tp, fp, fn, tn, reduction=\"micro\")\n",
    "\n",
    "def specificity(outputs: torch.Tensor, labels: torch.Tensor):\n",
    "    prob_mask = outputs.sigmoid()\n",
    "    pred_mask = (prob_mask > 0.5).float()\n",
    "    tp, fp, fn, tn = smp.metrics.get_stats(pred_mask.long(), labels.long(), mode=\"binary\")\n",
    "    return smp.metrics.specificity(tp, fp, fn, tn, reduction=\"micro\")\n",
    "\n",
    "def false_positive_rate(outputs: torch.Tensor, labels: torch.Tensor):\n",
    "    prob_mask = outputs.sigmoid()\n",
    "    pred_mask = (prob_mask > 0.5).float()\n",
    "    tp, fp, fn, tn = smp.metrics.get_stats(pred_mask.long(), labels.long(), mode=\"binary\")\n",
    "    return smp.metrics.false_positive_rate(tp, fp, fn, tn, reduction=\"micro\")\n",
    "\n",
    "def false_negative_rate(outputs: torch.Tensor, labels: torch.Tensor):\n",
    "    prob_mask = outputs.sigmoid()\n",
    "    pred_mask = (prob_mask > 0.5).float()\n",
    "    tp, fp, fn, tn = smp.metrics.get_stats(pred_mask.long(), labels.long(), mode=\"binary\")\n",
    "    return smp.metrics.false_negative_rate(tp, fp, fn, tn, reduction=\"micro\")\n",
    "\n",
    "def false_discovery_rate(outputs: torch.Tensor, labels: torch.Tensor):\n",
    "    prob_mask = outputs.sigmoid()\n",
    "    pred_mask = (prob_mask > 0.5).float()\n",
    "    tp, fp, fn, tn = smp.metrics.get_stats(pred_mask.long(), labels.long(), mode=\"binary\")\n",
    "    return smp.metrics.false_discovery_rate(tp, fp, fn, tn, reduction=\"micro\")\n",
    "\n",
    "def false_omission_rate(outputs: torch.Tensor, labels: torch.Tensor):\n",
    "    prob_mask = outputs.sigmoid()\n",
    "    pred_mask = (prob_mask > 0.5).float()\n",
    "    tp, fp, fn, tn = smp.metrics.get_stats(pred_mask.long(), labels.long(), mode=\"binary\")\n",
    "    return smp.metrics.false_omission_rate(tp, fp, fn, tn, reduction=\"micro\")\n",
    "\n",
    "def false_omission_rate(outputs: torch.Tensor, labels: torch.Tensor):\n",
    "    prob_mask = outputs.sigmoid()\n",
    "    pred_mask = (prob_mask > 0.5).float()\n",
    "    tp, fp, fn, tn = smp.metrics.get_stats(pred_mask.long(), labels.long(), mode=\"binary\")\n",
    "    return smp.metrics.false_omission_rate(tp, fp, fn, tn, reduction=\"micro\")\n",
    "\n",
    "def _handlezero_division_np(a,b):\n",
    "    # initialize output tensor with desired value\n",
    "    # c = torch.zeros_like(a)\n",
    "    #c = torch.full_like(a, fill_value=float('nan'))\n",
    "    # zero mask\n",
    "    c = np.zeros_like(a)\n",
    "    mask = (b != 0)\n",
    "    # finally perform division\n",
    "    c[mask] = a[mask] / b[mask]\n",
    "    return c\n",
    "\n",
    "def mathews_correlation_coefficient_np(tp, fp, fn, tn, eps=1e-11):\n",
    "    tp = tp.sum().astype(np.float64)\n",
    "    tn = tn.sum().astype(np.float64)\n",
    "    fp = fp.sum().astype(np.float64)\n",
    "    fn = fn.sum().astype(np.float64)\n",
    "    _numerator = (tp*tn - fp*fn)\n",
    "    _denomerator = np.sqrt((tp+fp)*(tp+fn)*(tn+fp)*(tn+fn))\n",
    "    x = _numerator / (_denomerator + eps)\n",
    "    x = _handlezero_division_np(_numerator, _denomerator)\n",
    "    return x\n",
    "\n",
    "class MCC_Loss(nn.Module):\n",
    "    \"\"\"\n",
    "    Calculates the proposed Matthews Correlation Coefficient-based loss.\n",
    "\n",
    "    Args:\n",
    "        inputs (torch.Tensor): 1-hot encoded predictions\n",
    "        targets (torch.Tensor): 1-hot encoded ground truth\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super(MCC_Loss, self).__init__()\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        \"\"\"\n",
    "        MCC = (TP.TN - FP.FN) / sqrt((TP+FP) . (TP+FN) . (TN+FP) . (TN+FN))\n",
    "        where TP, TN, FP, and FN are elements in the confusion matrix.\n",
    "        \"\"\"\n",
    "        tp, fp, fn, tn = smp.metrics.get_stats(inputs.long(), targets.long(), mode=\"binary\")\n",
    "        numerator = torch.mul(tp, tn) - torch.mul(fp, fn)\n",
    "        denominator = torch.sqrt(\n",
    "            torch.add(tp, 1, fp)\n",
    "            * torch.add(tp, 1, fn)\n",
    "            * torch.add(tn, 1, fp)\n",
    "            * torch.add(tn, 1, fn)\n",
    "        )\n",
    "\n",
    "        # Adding 1 to the denominator to avoid divide-by-zero errors.\n",
    "        # print(denominator.sum(), \"hoho -----------\")\n",
    "        if denominator.sum() == 0.0:\n",
    "            return torch.tensor(0.0, dtype=torch.float32)  # Return 0 to avoid division by zero\n",
    "        mcc = torch.div(numerator.sum(), denominator.sum() + 1.0)\n",
    "        # print(mcc ,\"------------------------\")\n",
    "        return 1 - mcc\n",
    "\n",
    "def mcc_cal_old(outputs: torch.Tensor, labels: torch.Tensor):\n",
    "    mcc_loss = MCC_Loss()\n",
    "    prob_mask = outputs.sigmoid()\n",
    "    pred_mask = (prob_mask > 0.5).float()\n",
    "    return 1 - mcc_loss(outputs.float(), labels)\n",
    "\n",
    "def mcc_cal(outputs: torch.Tensor, labels: torch.Tensor):\n",
    "    prob_mask = outputs.sigmoid()\n",
    "    pred_mask = (prob_mask > 0.5).float()\n",
    "    tp, fp, fn, tn = smp.metrics.get_stats(pred_mask.long(), labels.long(), mode=\"binary\")\n",
    "    tp, fp, fn, tn = tp.cpu().numpy(), fp.cpu().numpy(), fn.cpu().numpy(), tn.cpu().numpy()\n",
    "    return mathews_correlation_coefficient_np(tp, fp, fn, tn)\n",
    "\n",
    "# Funtion to create model\n",
    "# class CustomModelBase(pt_train.CustomModelBase):\n",
    "class CustomModelBase(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    ModelBase override for training and validation steps\n",
    "    \"\"\"\n",
    "    def __init__(self, class_weights=None, loss_function=dice_loss, accuracy_function=_accuracy, iou_function=_iou):\n",
    "        super(CustomModelBase, self).__init__()\n",
    "        self.class_weights = class_weights\n",
    "        self.loss_function = loss_function\n",
    "        self.accuracy_function = accuracy_function\n",
    "        self.iou_function = iou_function\n",
    "\n",
    "    def training_step(self, batch: list, forward_func: torch.nn.Module):\n",
    "        # using outer forward function as it is different on DataParallelization usage \n",
    "        images, labels = batch\n",
    "        out = forward_func(images)\n",
    "        loss = self.loss_function(out, labels)\n",
    "        acc = self.accuracy_function(out, labels)\n",
    "        iou = self.iou_function(out, labels)\n",
    "        return loss, acc, iou\n",
    "\n",
    "    def validation_step(self, batch: list, forward_func: torch.nn.Module):\n",
    "        # using outer forward function as it is different on DataParallelization usage\n",
    "        images, labels = batch\n",
    "        out = forward_func(images)\n",
    "        loss = self.loss_function(out, labels)\n",
    "        acc = self.accuracy_function(out, labels)\n",
    "        iou = self.iou_function(out, labels)\n",
    "        return {'val_loss': loss.detach(), 'val_acc': acc, 'val_iou': iou}\n",
    "\n",
    "    def validation_epoch_end(self, outputs):\n",
    "        batch_losses = [x['val_loss'] for x in outputs]\n",
    "        epoch_loss = torch.stack(batch_losses).mean()\n",
    "        batch_accs = [x['val_acc'] for x in outputs]\n",
    "        epoch_acc = torch.stack(batch_accs).mean()\n",
    "        batch_ious = [x['val_iou'] for x in outputs]\n",
    "        epoch_iou = torch.stack(batch_ious).mean()\n",
    "        return {'val_loss': epoch_loss.item(), 'val_acc': epoch_acc.item(), 'val_iou': epoch_iou.item()}\n",
    "\n",
    "    def epoch_end(self, epoch, result):\n",
    "        print(\n",
    "            f\"train_loss: {result['train_loss']:.4f}, val_loss: {result['val_loss']:.4f}\\n\"\n",
    "            f\"train_acc: {result['train_acc']:.4f}, val_acc: {result['val_acc']:.4f}\\n\"\n",
    "            f\"train_iou: {result['train_iou']:.4f}, val_iou: {result['val_iou']:.4f}\"\n",
    "        )\n",
    "        print()\n",
    "\n",
    "class CreateModel(CustomModelBase):\n",
    "    def __init__(self, model):\n",
    "        super(CreateModel, self).__init__()\n",
    "        self.model = model\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "\n",
    "def get_model(encoder_name, decoder_name):\n",
    "    if decoder_name == 'unet':\n",
    "        model = smp.Unet(encoder_name=encoder_name, encoder_weights=None, in_channels=3, classes=1)\n",
    "    elif decoder_name == 'unet++':\n",
    "        model = smp.UnetPlusPlus(encoder_name=encoder_name, encoder_weights=None, in_channels=3, classes=1)\n",
    "    elif decoder_name == 'pspnet':\n",
    "        model = smp.PSPNet(encoder_name=encoder_name, encoder_weights=None, in_channels=3, classes=1)\n",
    "    elif decoder_name == 'deeplabv3+':\n",
    "        model = smp.DeepLabV3Plus(encoder_name=encoder_name, encoder_weights=None, in_channels=3, classes=1)\n",
    "    elif decoder_name == 'deeplabv3':\n",
    "        model = smp.DeepLabV3(encoder_name=encoder_name, encoder_weights=None, in_channels=3, classes=1)\n",
    "    elif decoder_name == 'pan':\n",
    "        model = smp.PAN(encoder_name=encoder_name, encoder_weights=None, in_channels=3, classes=1)\n",
    "    return CreateModel(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-15T14:26:32.083296646Z",
     "start_time": "2023-05-15T14:26:32.065070653Z"
    },
    "id": "SP6pjJzJzmH5"
   },
   "outputs": [],
   "source": [
    "def save_results_and_plots(encoder_name, decoder_name, train_loss_history, train_acc_history, train_iou_history, val_loss_history, val_acc_history, val_iou_history):\n",
    "    results_df = pd.DataFrame({\n",
    "        'Epoch': list(range(1, len(train_loss_history) + 1)),\n",
    "        'Train Loss': train_loss_history,\n",
    "        'Val Loss': val_loss_history,\n",
    "        'Train Accuracy': train_acc_history,\n",
    "        'Val Accuracy': val_acc_history,\n",
    "        'Train IOU': train_iou_history,\n",
    "        'Val IOU': val_iou_history\n",
    "    })\n",
    "\n",
    "    results_df.to_excel(f'results_{encoder_name}_{decoder_name}.xlsx', index=False)\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(results_df['Epoch'], results_df['Train Loss'], label='Train Loss')\n",
    "    plt.plot(results_df['Epoch'], results_df['Val Loss'], label='Val Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.title(f'Loss Curves for {encoder_name} with {decoder_name}')\n",
    "    plt.savefig(f'loss_curves_{encoder_name}_{decoder_name}.png')\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(results_df['Epoch'], results_df['Train Accuracy'], label='Train Accuracy')\n",
    "    plt.plot(results_df['Epoch'], results_df['Val Accuracy'], label='Val Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    plt.title(f'Accuracy Curves for {encoder_name} with {decoder_name}')\n",
    "    plt.savefig(f'accuracy_curves_{encoder_name}_{decoder_name}.png')\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(results_df['Epoch'], results_df['Train IOU'], label='Train IOU')\n",
    "    plt.plot(results_df['Epoch'], results_df['Val IOU'], label='Val IOU')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('IOU')\n",
    "    plt.legend()\n",
    "    plt.title(f'IOU Curves for {encoder_name} with {decoder_name}')\n",
    "    plt.savefig(f'iou_curves_{encoder_name}_{decoder_name}.png')\n",
    "\n",
    "    # metrics_df = pd.DataFrame([metrics])\n",
    "    # metrics_df.to_excel(f'metrics_{encoder_name}_{decoder_name}.xlsx', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-15T14:27:32.843180644Z",
     "start_time": "2023-05-15T14:27:32.798602876Z"
    },
    "id": "B7Y8z2bk-RiV"
   },
   "outputs": [],
   "source": [
    "import pytorch_utils.callbacks as pt_callbacks\n",
    "import pytorch_utils.training_utils as pt_train\n",
    "\n",
    "from torch.optim import Adam as adam_opt\n",
    "import traceback\n",
    "\n",
    "def fit(\n",
    "        epochs: int,\n",
    "        lr: float,\n",
    "        weight_decay: float,\n",
    "        model: torch.nn.Module,\n",
    "        train_loader: torch.utils.data.DataLoader,\n",
    "        val_loader: torch.utils.data.DataLoader,\n",
    "        callbacks_function=None,\n",
    "        continue_training=False,\n",
    "        opt_func=adam_opt,\n",
    "        device=device,\n",
    "        num_retries_inner=10,\n",
    "        max_retry=10,\n",
    "        evaluate=_evaluate\n",
    "):\n",
    "    \"\"\"\n",
    "    Meant to resemble the fit function in keras.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    epochs - Set this to a high number and use callbacks to stop training early\n",
    "    lr - Initial learning rate in case of a scheduler\n",
    "    weight_decay - Weight decay to be fed to the optimizer\n",
    "    model - The model to train. Must inherit from CustomModelBase of this module\n",
    "    train_loader - The training data loader\n",
    "    val_loader - The validation data loader\n",
    "    callbacks_function - A function that takes the model and returns a list of callbacks\n",
    "    opt_func - The optimizer function to use\n",
    "    device - The device to use\n",
    "    num_retries_inner - Number of times to retry training if it fails\n",
    "    max_retry - Maximum number of times to retry training if anything other than training_step fails, like dataloader\n",
    "    evaluate - The function to use for evaluation, defaults to _evaluate()\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    history - A list of dictionaries containing the loss and accuracy for each epoch\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    if weight_decay is not None:\n",
    "        optimizer = opt_func(model.parameters(), lr, weight_decay=weight_decay)\n",
    "    else:\n",
    "        optimizer = opt_func(model.parameters(), lr)\n",
    "\n",
    "    # model.to(device)\n",
    "    defined_callbacks = None  # must be None so that it can be defined in the function when it is called for the first time\n",
    "    num_retry = 0\n",
    "    history = []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()  # Make sure the model is in training mode at each epoch, because it is set to eval() in evaluate()\n",
    "        train_losses = []\n",
    "        accuracies = []\n",
    "        ious = []\n",
    "        print(\"LR: \", optimizer.param_groups[0]['lr'])\n",
    "        # Wrap the train_loader with tqdm to create a progress bar\n",
    "\n",
    "        while num_retry < max_retry:\n",
    "            try:\n",
    "                progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch + 1}\", delay=1)\n",
    "\n",
    "                for batch in progress_bar:\n",
    "                    batch = [tensor.to(device).float() for tensor in batch]\n",
    "\n",
    "                    # run the training step many times until it works\n",
    "                    flag = False\n",
    "                    for i in range(num_retries_inner):\n",
    "                        try:\n",
    "                            if USE_2_GPUS:\n",
    "                                loss, acc, iou = model.module.training_step(batch, model)\n",
    "                            else:\n",
    "                                loss, acc, iou = model.training_step(batch, model)\n",
    "                                \n",
    "                            flag = True\n",
    "                            break\n",
    "                        except:\n",
    "                            if i == num_retries_inner - 1:\n",
    "                                traceback.print_exc()\n",
    "\n",
    "                            # try cleaning the cache\n",
    "                            torch.cuda.empty_cache()\n",
    "                            gc.collect()\n",
    "\n",
    "                    if not flag:\n",
    "                        raise RuntimeError(f\"Training step failed {num_retries_inner} times\")\n",
    "\n",
    "                    train_losses.append(loss)\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                    optimizer.zero_grad()\n",
    "\n",
    "                    accuracies.append(acc)\n",
    "                    ious.append(iou)\n",
    "\n",
    "                    # rounded_loss = round(loss.item(), 3)\n",
    "\n",
    "                    # progress_bar.set_postfix(loss=rounded_loss, accuracy=acc.item())\n",
    "                    # Update the progress bar with the current loss and accuracy\n",
    "                    progress_bar.set_postfix(loss=loss.item(), accuracy=acc.item())\n",
    "\n",
    "                num_retry = 0\n",
    "                break\n",
    "            except:\n",
    "                # try cleaning the cache\n",
    "                torch.cuda.empty_cache()\n",
    "                gc.collect()\n",
    "\n",
    "                num_retry += 1\n",
    "                if num_retry < max_retry:\n",
    "                    continue\n",
    "                else:\n",
    "                    traceback.print_exc()\n",
    "                    raise RuntimeError(f\"Training failed {max_retry} times\")\n",
    "\n",
    "        result = evaluate(model, val_loader, device)\n",
    "        result['train_loss'] = torch.stack(train_losses).mean().item()\n",
    "        result['train_acc'] = torch.stack(accuracies).mean().item()\n",
    "        result['train_iou'] = torch.stack(ious).mean().item()\n",
    "\n",
    "        if callbacks_function is not None:\n",
    "            defined_callbacks, stop_flag = callbacks_function(\n",
    "                optimiser=optimizer,\n",
    "                result=result,\n",
    "                model=model,\n",
    "                defined_callbacks=defined_callbacks,\n",
    "                continue_training=continue_training,\n",
    "            )\n",
    "            if USE_2_GPUS:\n",
    "                model.module.epoch_end(epoch, result)\n",
    "            else:\n",
    "                model.epoch_end(epoch, result)\n",
    "            history.append(result)\n",
    "\n",
    "            if stop_flag:\n",
    "                print(\"Early stopping triggered\")\n",
    "                break\n",
    "\n",
    "    return history\n",
    "\n",
    "\n",
    "def train_loop(\n",
    "        model,\n",
    "        encoder,\n",
    "        decoder,\n",
    "        optimizer,\n",
    "        epochs,\n",
    "        train_loader,\n",
    "        val_loader,\n",
    "        device=device,\n",
    "        initial_lr=None,\n",
    "        weight_decay=None,\n",
    "        verbose=False,\n",
    "        save=False\n",
    "):\n",
    "    def get_result_list(history, metric):\n",
    "        return [history[i][metric] for i in range(len(history))]\n",
    "\n",
    "    # Train the model using torch\n",
    "\n",
    "    def get_callbacks(\n",
    "        # model_save_path,\n",
    "        optimiser,\n",
    "        result,\n",
    "        model,\n",
    "        defined_callbacks=None,\n",
    "        continue_training=False,\n",
    "        other_stats=None):\n",
    "\n",
    "        model_save_path = '/'.format(encoder, decoder)\n",
    "        os.makedirs(model_save_path, exist_ok=True)\n",
    "\n",
    "        if defined_callbacks is None:\n",
    "            defined_callbacks = {\n",
    "                'val': pt_callbacks.Callbacks(optimizer=optimiser,\n",
    "                                              model_save_path=model_save_path + 'best_model.pth',\n",
    "                                              training_stats_path=model_save_path + 'training_stats_val',\n",
    "                                              continue_training=continue_training),\n",
    "\n",
    "                'train': pt_callbacks.Callbacks(optimizer=optimiser,\n",
    "                                                training_stats_path=model_save_path + 'training_stats_train',\n",
    "                                                continue_training=continue_training)\n",
    "            }\n",
    "\n",
    "        defined_callbacks['val'].reduce_lr_on_plateau(\n",
    "            monitor_value=result[\"val_iou\"],\n",
    "            mode='max',\n",
    "            factor=0.5,\n",
    "            patience=5,\n",
    "            indicator_text=\"Val LR scheduler: \"\n",
    "        )\n",
    "        defined_callbacks['val'].model_checkpoint(\n",
    "            model=model,\n",
    "            monitor_value=result[\"val_iou\"],\n",
    "            mode='max',\n",
    "            indicator_text=\"Val checkpoint: \"\n",
    "        )\n",
    "        stop_flag = defined_callbacks['val'].early_stopping(\n",
    "            monitor_value=result[\"val_iou\"],\n",
    "            mode='max',\n",
    "            patience=20,\n",
    "            indicator_text=\"Early stopping: \"\n",
    "        )\n",
    "        defined_callbacks['val'].clear_memory()\n",
    "        print(\"_________\")\n",
    "\n",
    "        return defined_callbacks, stop_flag\n",
    "\n",
    "    def get_callbacks_no_save(\n",
    "            # model_save_path,\n",
    "            optimiser,\n",
    "            result,\n",
    "            model,\n",
    "            defined_callbacks=None,\n",
    "            continue_training=False,\n",
    "            other_stats=None):\n",
    "\n",
    "        model_save_path = ''\n",
    "\n",
    "        if defined_callbacks is None:\n",
    "            defined_callbacks = {\n",
    "                'val': pt_callbacks.Callbacks(optimizer=optimiser,\n",
    "                                              model_save_path=model_save_path + 'best_model.pth',\n",
    "                                              training_stats_path=model_save_path + 'training_stats_val',\n",
    "                                              continue_training=continue_training),\n",
    "\n",
    "                'train': pt_callbacks.Callbacks(optimizer=optimiser,\n",
    "                                                training_stats_path=model_save_path + 'training_stats_train',\n",
    "                                                continue_training=continue_training)\n",
    "            }\n",
    "\n",
    "        defined_callbacks['val'].reduce_lr_on_plateau(\n",
    "            monitor_value=result[\"val_iou\"],\n",
    "            mode='max',\n",
    "            factor=0.5,\n",
    "            patience=5,\n",
    "            indicator_text=\"Val LR scheduler: \"\n",
    "        )\n",
    "        defined_callbacks['val'].model_checkpoint(\n",
    "            model=model,\n",
    "            monitor_value=result[\"val_iou\"],\n",
    "            mode='max',\n",
    "            indicator_text=\"Val checkpoint: \"\n",
    "        )\n",
    "        stop_flag = defined_callbacks['val'].early_stopping(\n",
    "            monitor_value=result[\"val_iou\"],\n",
    "            mode='max',\n",
    "            patience=20,\n",
    "            indicator_text=\"Early stopping: \"\n",
    "        )\n",
    "        defined_callbacks['val'].clear_memory()\n",
    "        print(\"_________\")\n",
    "\n",
    "        return defined_callbacks, stop_flag\n",
    "\n",
    "    if not save:\n",
    "        history = fit(\n",
    "            epochs=epochs,\n",
    "            lr=initial_lr,\n",
    "            weight_decay=weight_decay,\n",
    "            model=model,\n",
    "            device=device,\n",
    "            callbacks_function=get_callbacks_no_save,\n",
    "            train_loader=train_loader,\n",
    "            val_loader=val_loader,\n",
    "            opt_func=optimizer,\n",
    "        )\n",
    "        del model\n",
    "        model_save_path = ''\n",
    "        os.makedirs(model_save_path, exist_ok=True)\n",
    "        # load the best model from checkpoint\n",
    "        model = torch.load(model_save_path + \"best_model.pth\")\n",
    "    else:\n",
    "        history = fit(\n",
    "            epochs=epochs,\n",
    "            lr=initial_lr,\n",
    "            weight_decay=weight_decay,\n",
    "            model=model,\n",
    "            device=device,\n",
    "            callbacks_function=get_callbacks,\n",
    "            train_loader=train_loader,\n",
    "            val_loader=val_loader,\n",
    "            opt_func=optimizer,\n",
    "        )\n",
    "\n",
    "        del model\n",
    "        model_save_path = ''.format(encoder, decoder)\n",
    "        os.makedirs(model_save_path, exist_ok=True)\n",
    "        # load the best model from checkpoint\n",
    "        model = torch.load(model_save_path + \"best_model.pth\")\n",
    "\n",
    "    train_loss_history = get_result_list(history, \"train_loss\")\n",
    "    train_acc_history = get_result_list(history, \"train_acc\")\n",
    "    train_iou_history = get_result_list(history, \"train_iou\")\n",
    "    val_loss_history = get_result_list(history, \"val_loss\")\n",
    "    val_acc_history = get_result_list(history, \"val_acc\")\n",
    "    val_iou_history = get_result_list(history, \"val_iou\")\n",
    "\n",
    "    return model, train_loss_history, train_acc_history, train_iou_history, val_loss_history, val_acc_history, val_iou_history\n",
    "\n",
    "\n",
    "def train_model(model, encoder, decoder, train_loader, val_loader, device, kwargs):\n",
    "\n",
    "    print(kwargs)    y_true = torch.cat(y_true)\n",
    "    y_pred = torch.cat(y_pred)\n",
    "    y_pred_binary = (y_pred > 0.5).float()\n",
    "    y_true_binary = (y_true > 0.5).float()\n",
    "\n",
    "    epochs = kwargs.get(\"epochs\")\n",
    "    \n",
    "    model = model.to(device)\n",
    "    save = kwargs.get(\"save\")\n",
    "    # get optimizer\n",
    "    optim_args = kwargs.get(\"optim\")\n",
    "    print(optim_args[\"params\"])\n",
    "    optimizer_cls = get_optimizer_by_name(optim_args.get(\"name\"))\n",
    "    optimizer = optimizer_cls(model.parameters(), **optim_args[\"params\"])\n",
    "    lr = optim_args[\"params\"][\"lr\"]\n",
    "    weight_decay = optim_args[\"params\"][\"weight_decay\"]\n",
    "    model, train_loss_history, train_acc_history, train_iou_history, val_loss_history, val_acc_history, val_iou_history = train_loop(model, encoder, decoder, optimizer_cls, epochs, train_loader, val_loader, device=device, initial_lr=lr, weight_decay=weight_decay, verbose=True, save=save)\n",
    "\n",
    "    return model, train_loss_history, train_acc_history, train_iou_history, val_loss_history, val_acc_history, val_iou_history\n",
    "\n",
    "mcc_loss = MCC_Loss()\n",
    "\n",
    "def evaluate_model(model, test_loader, device, verbose=True, eps=1e-10):\n",
    "    if verbose:\n",
    "        print('--------------------------------------------')\n",
    "        print('Test metrics (on test set)')\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, masks in test_loader:\n",
    "            inputs, masks = inputs.to(device).float(), masks.to(device).float()\n",
    "            outputs = model(inputs)\n",
    "            outputs = outputs.sigmoid()\n",
    "            y_true.append(masks.cpu())\n",
    "            y_pred.append(outputs.cpu())\n",
    "\n",
    "    y_true = torch.cat(y_true)\n",
    "    y_pred = torch.cat(y_pred)\n",
    "    metrics = {\n",
    "        'Overall Accuracy': round(_accuracy(y_true, y_pred).item() * 100, 2),\n",
    "        'Precision': round(precision_score(y_true, y_pred).item(), 2),\n",
    "        'Recall': round(recall_score(y_true, y_pred).item(), 2),\n",
    "        'F1 Score': round(f1_score(y_true, y_pred).item(), 2),\n",
    "        'Specificity': round(recall_score(y_true, y_pred).item(), 2),\n",
    "        'False Positive Rate': round(false_positive_rate(y_true, y_pred).item(), 4),\n",
    "        'False Negative Rate': round(false_negative_rate(y_true, y_pred).item(), 4),\n",
    "        'False Discovery Rate': round(false_discovery_rate(y_true, y_pred).item(), 4),\n",
    "        'False Omission Rate': round(false_omission_rate(y_true, y_pred).item(), 7),\n",
    "        'Misclassification Rate': round(1 - _accuracy(y_true, y_pred).item(), 2),\n",
    "        'Matthew\\'s Correlation Coefficient': round(mcc_cal(y_true, y_pred).item(), 2),\n",
    "        'IoU (Jaccard Index)': round(_iou(y_true, y_pred).item(), 2),\n",
    "        'IoU_plant': round(_iou_plant(y_true, y_pred).item(), 2),\n",
    "        'Geometric Mean': round(np.sqrt(precision_score(y_true, y_pred).item() * recall_score(y_true, y_pred).item()), 2)\n",
    "    }\n",
    "    print('--------------------------------------------')\n",
    "    print()\n",
    "\n",
    "    return metrics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FGtFqxR6VSXw"
   },
   "source": [
    "## Train non-transformer modles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HZvSJzDVsDFo"
   },
   "source": [
    "### Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "ZjEbpFT1sDFo",
    "outputId": "568b8bd9-edf3-4385-be1b-b8b736efc93a"
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Load existing results from Excel file if it exists\n",
    "excel_file = ''\n",
    "\n",
    "kwargs = {'epochs': 10, 'optim': {'name': 'Adam', 'params': {\n",
    "     'lr': 0.01, 'weight_decay': 3.310305423548208e-05}}, 'save': False}\n",
    "\n",
    "try:\n",
    "    existing_df = pd.read_excel(excel_file)\n",
    "    results = existing_df.to_dict(orient='list')\n",
    "    processed_models = list(existing_df.apply(lambda row: f\"{row['Encoder']}--{row['Decoder']}\", axis=1))\n",
    "except FileNotFoundError:\n",
    "    print(\"Experiment from the begining!!!\")\n",
    "    processed_models = []\n",
    "# Initialize lists to store metrics\n",
    "    results = {\n",
    "        'Encoder': [],\n",
    "        'Decoder': [],\n",
    "        'Train Loss': [],\n",
    "        'Train Accuracy': [],\n",
    "        'Train IoU': [],\n",
    "        'Val Loss': [],\n",
    "        'Val Accuracy': [],\n",
    "        'Val IoU': [],\n",
    "        'Overall Accuracy': [],\n",
    "        'Precision': [],\n",
    "        'Recall': [],\n",
    "        'F1 Score': [],\n",
    "        'Specificity': [],\n",
    "        'False Positive Rate': [],\n",
    "        'False Negative Rate': [],\n",
    "        'False Discovery Rate': [],\n",
    "        'False Omission Rate': [],\n",
    "        'Misclassification Rate': [],\n",
    "        'Matthew\\'s Correlation Coefficient': [],\n",
    "        'IoU (Jaccard Index)': [],\n",
    "        'IoU_plant' : [],\n",
    "        'Geometric Mean': []\n",
    "    }\n",
    "\n",
    "for encoder_name in encoders:\n",
    "    for decoder_name in decoders:\n",
    "        model_info = f\"{encoder_name}--{decoder_name}\"\n",
    "        if model_info in processed_models:\n",
    "            print(f'Skipping {model_info}, already processed.')\n",
    "            continue\n",
    "        print(f'Training {encoder_name} with {decoder_name}')\n",
    "        \n",
    "        if USE_2_GPUS:\n",
    "            model = get_model(encoder_name, decoder_name).cuda()\n",
    "            # model = nn.DataParallel(model).to(device)\n",
    "            model = nn.DataParallel(model, device_ids=[0,1])\n",
    "        else:\n",
    "            model = get_model(encoder_name, decoder_name).to(device)\n",
    "\n",
    "        # Train the model and collect metrics\n",
    "        model, train_loss_history, train_acc_history, train_iou_history, val_loss_history, val_acc_history, val_iou_history = train_model(model, encoder_name, decoder_name, train_loader, val_loader, device, kwargs)\n",
    "        # save_results_and_plots(encoder_name, decoder_name, train_loss_history, train_acc_history, train_iou_history, val_loss_history, val_acc_history, val_iou_history)\n",
    "        ## show plots\n",
    "        results_df = pd.DataFrame({\n",
    "            'Epoch': list(range(1, len(train_loss_history) + 1)),\n",
    "            'Train Loss': train_loss_history,\n",
    "            'Val Loss': val_loss_history,\n",
    "            'Train Accuracy': train_acc_history,\n",
    "            'Val Accuracy': val_acc_history,\n",
    "            'Train IOU': train_iou_history,\n",
    "            'Val IOU': val_iou_history\n",
    "        })\n",
    "        plt.figure()\n",
    "        plt.plot(results_df['Epoch'], results_df['Train Loss'], label='Train Loss')\n",
    "        plt.plot(results_df['Epoch'], results_df['Val Loss'], label='Val Loss')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.legend()\n",
    "        plt.title(f'Loss Curves for {encoder_name} with {decoder_name}')\n",
    "        plt.savefig(f'loss_curves_{encoder_name}_{decoder_name}.png')\n",
    "        plt.show()\n",
    "\n",
    "        plt.figure()\n",
    "        plt.plot(results_df['Epoch'], results_df['Train Accuracy'], label='Train Accuracy')\n",
    "        plt.plot(results_df['Epoch'], results_df['Val Accuracy'], label='Val Accuracy')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Accuracy')\n",
    "        plt.legend()\n",
    "        plt.title(f'Accuracy Curves for {encoder_name} with {decoder_name}')\n",
    "        plt.savefig(f'accuracy_curves_{encoder_name}_{decoder_name}.png')\n",
    "        plt.show()\n",
    "\n",
    "        plt.figure()\n",
    "        plt.plot(results_df['Epoch'], results_df['Train IOU'], label='Train IOU')\n",
    "        plt.plot(results_df['Epoch'], results_df['Val IOU'], label='Val IOU')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('IOU')\n",
    "        plt.legend()\n",
    "        plt.title(f'IOU Curves for {encoder_name} with {decoder_name}')\n",
    "        plt.savefig(f'iou_curves_{encoder_name}_{decoder_name}.png')\n",
    "        plt.show()\n",
    "\n",
    "        # Evaluate on the test set\n",
    "        test_metrics = evaluate_model(model, test_loader, device=device)\n",
    "        for key, value in test_metrics.items():\n",
    "            if key in [\"False Discovery Rate\", \"False Negative Rate\", \"False Positive Rate\"]:\n",
    "                print(f'\"{key}\": {value:.4f},')\n",
    "            elif key == \"False Omission Rate\":\n",
    "                print(f'\"{key}\": {value:.7f},')\n",
    "            else:\n",
    "                print(f'\"{key}\": {value:.2f},')\n",
    "\n",
    "        # Save metrics to results dictionary\n",
    "        results['Encoder'].append(encoder_name)\n",
    "        results['Decoder'].append(decoder_name)\n",
    "        results['Train Loss'].append(np.min(train_loss_history))\n",
    "        results['Train Accuracy'].append(np.max(train_acc_history))\n",
    "        results['Train IoU'].append(np.max(train_iou_history))\n",
    "        results['Val Loss'].append(np.min(val_loss_history))\n",
    "        results['Val Accuracy'].append(np.max(val_acc_history))\n",
    "        results['Val IoU'].append(np.max(val_iou_history))\n",
    "        for metric_name, metric_value in test_metrics.items():\n",
    "            results[metric_name].append(metric_value)\n",
    "\n",
    "        \n",
    "        # Convert results to pandas DataFrame\n",
    "        df_new = pd.DataFrame(results)\n",
    "        \n",
    "        # Load existing DataFrame\n",
    "        try:\n",
    "            df_existing = pd.read_excel(excel_file)\n",
    "            df_combined = pd.concat([df_existing, df_new]).drop_duplicates().reset_index(drop=True)\n",
    "        except FileNotFoundError:\n",
    "            df_combined = df_new\n",
    "        \n",
    "        # Save DataFrame to Excel\n",
    "        df_combined.to_excel(excel_file, index=False)\n",
    "        \n",
    "        print(f'Results saved to {excel_file}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d0iZFID_sDFo"
   },
   "source": [
    "### Train 2 best models + save pt files + test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "7DiwYFC3sDFo",
    "outputId": "f807c9bf-3e36-4f86-a402-785dbf3d58f8"
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Enter your best encoder-decoder combination here\n",
    "best_models = ['inceptionv4--unet']\n",
    "\n",
    "kwargs = {'epochs': 10000, 'optim': {'name': 'Adam', 'params': {\n",
    "     'lr': 0.01, 'weight_decay': 3.310305423548208e-05}}, 'save': True}\n",
    "\n",
    "# Load existing results from Excel file if it exists\n",
    "excel_file = ''\n",
    "\n",
    "try:\n",
    "    existing_df = pd.read_excel(excel_file)\n",
    "    results = existing_df.to_dict(orient='list')\n",
    "    processed_models = list(existing_df.apply(lambda row: f\"{row['Encoder']}--{row['Decoder']}\", axis=1))\n",
    "except FileNotFoundError:\n",
    "    print(\"Experiment from the begining!!!\")\n",
    "    processed_models = []\n",
    "# Initialize lists to store metrics\n",
    "    results = {\n",
    "        'Encoder': [],\n",
    "        'Decoder': [],\n",
    "        'Train Loss': [],\n",
    "        'Train Accuracy': [],\n",
    "        'Train IoU': [],\n",
    "        'Val Loss': [],\n",
    "        'Val Accuracy': [],\n",
    "        'Val IoU': [],\n",
    "        'Overall Accuracy': [],\n",
    "        'Precision': [],\n",
    "        'Recall': [],\n",
    "        'F1 Score': [],\n",
    "        'Specificity': [],\n",
    "        'False Positive Rate': [],\n",
    "        'False Negative Rate': [],\n",
    "        'False Discovery Rate': [],\n",
    "        'False Omission Rate': [],\n",
    "        'Misclassification Rate': [],\n",
    "        'Matthew\\'s Correlation Coefficient': [],\n",
    "        'IoU (Jaccard Index)': [],\n",
    "        'IoU_plant' : [],\n",
    "        'Geometric Mean': []\n",
    "    }\n",
    "\n",
    "for model_info in best_models:\n",
    "    if model_info in processed_models:\n",
    "        print(f'Skipping {model_info}, already processed.')\n",
    "        continue\n",
    "    encoder_name, decoder_name = model_info.split('--')\n",
    "    print(f'Training {encoder_name} with {decoder_name}')\n",
    "    if USE_2_GPUS:\n",
    "        model = get_model(encoder_name, decoder_name)\n",
    "        model = nn.DataParallel(model).to(device)\n",
    "    else:\n",
    "        model = get_model(encoder_name, decoder_name).to(device)\n",
    "\n",
    "    # # Train the model and collect metrics\n",
    "    model, train_loss_history, train_acc_history, train_iou_history, val_loss_history, val_acc_history, val_iou_history = train_model(model, encoder_name, decoder_name, train_loader, val_loader, device, kwargs)\n",
    "\n",
    "    # # Load the best model state\n",
    "    # model.load_state_dict(torch.load(f'best_model_{encoder_name}_{decoder_name}.pth'))\n",
    "    # model.load_state_dict(torch.load(f'/content/model/{encoder_name}-{decoder_name}/model.pth'))\n",
    "    ## show plots\n",
    "    results_df = pd.DataFrame({\n",
    "        'Epoch': list(range(1, len(train_loss_history) + 1)),\n",
    "        'Train Loss': train_loss_history,\n",
    "        'Val Loss': val_loss_history,\n",
    "        'Train Accuracy': train_acc_history,\n",
    "        'Val Accuracy': val_acc_history,\n",
    "        'Train IOU': train_iou_history,\n",
    "        'Val IOU': val_iou_history\n",
    "    })\n",
    "    plt.figure()\n",
    "    plt.plot(results_df['Epoch'], results_df['Train Loss'], label='Train Loss', marker=11)\n",
    "    plt.plot(results_df['Epoch'], results_df['Val Loss'], label='Val Loss', marker=11)\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.title(f'Loss Curves for {encoder_name} with {decoder_name}')\n",
    "    plt.savefig(f'loss_curves_{encoder_name}_{decoder_name}.png')\n",
    "    plt.show()\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(results_df['Epoch'], results_df['Train Accuracy'], label='Train Accuracy', marker=11)\n",
    "    plt.plot(results_df['Epoch'], results_df['Val Accuracy'], label='Val Accuracy', marker=11)\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    plt.title(f'Accuracy Curves for {encoder_name} with {decoder_name}')\n",
    "    plt.savefig(f'accuracy_curves_{encoder_name}_{decoder_name}.png')\n",
    "    plt.show()\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(results_df['Epoch'], results_df['Train IOU'], label='Train IOU', marker=11)\n",
    "    plt.plot(results_df['Epoch'], results_df['Val IOU'], label='Val IOU', marker=11)\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('IOU')\n",
    "    plt.legend()\n",
    "    plt.title(f'IOU Curves for {encoder_name} with {decoder_name}')\n",
    "    plt.savefig(f'iou_curves_{encoder_name}_{decoder_name}.png')\n",
    "    plt.show()\n",
    "\n",
    "    x = model\n",
    "\n",
    "    # Evaluate on the test set\n",
    "    test_metrics = evaluate_model(x, test_loader, device=device)\n",
    "    for key, value in test_metrics.items():\n",
    "        if key in [\"False Discovery Rate\", \"False Negative Rate\", \"False Positive Rate\"]:\n",
    "            print(f'\"{key}\": {value:.4f}')\n",
    "        elif key == \"False Omission Rate\":\n",
    "            print(f'\"{key}\": {value:.7f}')\n",
    "        else:\n",
    "            print(f'\"{key}\": {value:.2f}')\n",
    "    print('--------------------------------------------')\n",
    "    # Append metrics to results dictionary\n",
    "    results['Encoder'].append(encoder_name)\n",
    "    results['Decoder'].append(decoder_name)\n",
    "    results['Train Loss'].append(np.min(train_loss_history))\n",
    "    results['Train Accuracy'].append(np.max(train_acc_history))\n",
    "    results['Train IoU'].append(np.max(train_iou_history))\n",
    "    results['Val Loss'].append(np.min(val_loss_history))\n",
    "    results['Val Accuracy'].append(np.max(val_acc_history))\n",
    "    results['Val IoU'].append(np.max(val_iou_history))\n",
    "    for metric_name, metric_value in test_metrics.items():\n",
    "        results[metric_name].append(metric_value)\n",
    "    processed_models.append(model_info)\n",
    "\n",
    "# Convert results to pandas DataFrame\n",
    "df_new = pd.DataFrame(results)\n",
    "\n",
    "# Load existing DataFrame\n",
    "try:\n",
    "    df_existing = pd.read_excel(excel_file)\n",
    "    df_combined = pd.concat([df_existing, df_new]).drop_duplicates().reset_index(drop=True)\n",
    "except FileNotFoundError:\n",
    "    df_combined = df_new\n",
    "\n",
    "# Save DataFrame to Excel\n",
    "df_combined.to_excel(excel_file, index=False)\n",
    "\n",
    "print(f'Results saved to {excel_file}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bYBiq2NGVJqw"
   },
   "source": [
    "### Show some results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 462
    },
    "id": "mPFQ9fCKlHBr",
    "outputId": "32731ed1-e9a8-41db-c7c8-3a675f6fff47"
   },
   "outputs": [],
   "source": [
    "next(iter(test_loader))\n",
    "images, masks = next(iter(test_loader))\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    logits = model(images.to(device).float())\n",
    "pr_masks = logits.sigmoid()\n",
    "\n",
    "for image, gt_mask, pr_mask in zip(images, masks, pr_masks):\n",
    "    plt.figure(figsize=(10, 5))\n",
    "\n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.imshow(image.numpy().transpose(1, 2, 0))  # convert CHW -> HWC\n",
    "    plt.title(\"Image\")\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "    plt.subplot(1, 3, 2)\n",
    "    plt.imshow(gt_mask.numpy().squeeze()) # just squeeze classes dim, because we have only one class\n",
    "    plt.title(\"Ground truth\")\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "    plt.subplot(1, 3, 3)\n",
    "    plt.imshow(pr_mask.cpu().numpy().squeeze()) # just squeeze classes dim, because we have only one class\n",
    "    plt.title(\"Prediction\")\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9D-E2_J6wrCV"
   },
   "source": [
    "# Segformer Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_v2(encoder_name, decoder_name):\n",
    "    if decoder_name == 'unet':\n",
    "        model = smp.Unet(encoder_name=encoder_name, encoder_weights=None, in_channels=3, classes=1)\n",
    "    elif decoder_name == 'unet++':\n",
    "        model = smp.UnetPlusPlus(encoder_name=encoder_name, encoder_weights=None, in_channels=3, classes=1)\n",
    "    elif decoder_name == 'pspnet':\n",
    "        model = smp.PSPNet(encoder_name=encoder_name, encoder_weights=None, in_channels=3, classes=1)\n",
    "    elif decoder_name == 'deeplabv3+':\n",
    "        model = smp.DeepLabV3Plus(encoder_name=encoder_name, encoder_weights=None, in_channels=3, classes=1)\n",
    "    elif decoder_name == 'deeplabv3':\n",
    "        model = smp.DeepLabV3(encoder_name=encoder_name, encoder_weights=None, in_channels=3, classes=1)\n",
    "    return CreateModel(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "infHfqSbwu7w",
    "outputId": "be31e8bc-34c7-4b83-cf14-1550a7c2c4c2"
   },
   "outputs": [],
   "source": [
    "encoders = ['mit_b1', 'mit_b3', 'mit_b5']\n",
    "decoders = ['unet', 'unet++', 'pspnet', 'deeplabv3+']\n",
    "\n",
    "print(f\"number of encoders {len(encoders)}\")\n",
    "print(f\"number of decoders {len(decoders)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Load existing results from Excel file if it exists\n",
    "excel_file = ''\n",
    "\n",
    "kwargs = {'epochs': 10000, 'optim': {'name': 'Adam', 'params': {\n",
    "     'lr': 0.01, 'weight_decay': 3.310305423548208e-05}}, 'save': False}\n",
    "\n",
    "try:\n",
    "    existing_df = pd.read_excel(excel_file)\n",
    "    results = existing_df.to_dict(orient='list')\n",
    "    processed_models = list(existing_df.apply(lambda row: f\"{row['Encoder']}--{row['Decoder']}\", axis=1))\n",
    "except FileNotFoundError:\n",
    "    print(\"Experiment from the begining!!!\")\n",
    "    processed_models = []\n",
    "# Initialize lists to store metrics\n",
    "    results = {\n",
    "        'Encoder': [],\n",
    "        'Decoder': [],\n",
    "        'Train Loss': [],\n",
    "        'Train Accuracy': [],\n",
    "        'Train IoU': [],\n",
    "        'Val Loss': [],\n",
    "        'Val Accuracy': [],\n",
    "        'Val IoU': [],\n",
    "        'Overall Accuracy': [],\n",
    "        'Precision': [],\n",
    "        'Recall': [],\n",
    "        'F1 Score': [],\n",
    "        'Specificity': [],\n",
    "        'False Positive Rate': [],\n",
    "        'False Negative Rate': [],\n",
    "        'False Discovery Rate': [],\n",
    "        'False Omission Rate': [],\n",
    "        'Misclassification Rate': [],\n",
    "        'Matthew\\'s Correlation Coefficient': [],\n",
    "        'IoU (Jaccard Index)': [],\n",
    "        'IoU (Jaccard Index)': [],\n",
    "        'Geometric Mean': []\n",
    "    }\n",
    "\n",
    "for encoder_name in encoders:\n",
    "    for decoder_name in decoders:\n",
    "        model_info = f\"{encoder_name}--{decoder_name}\"\n",
    "        if model_info in processed_models:\n",
    "            print(f'Skipping {model_info}, already processed.')\n",
    "            continue\n",
    "        print(f'Training {encoder_name} with {decoder_name}')\n",
    "        \n",
    "        if USE_2_GPUS:\n",
    "            model = get_model_v2(encoder_name, decoder_name).cuda()\n",
    "            # model = nn.DataParallel(model).to(device)\n",
    "            model = nn.DataParallel(model, device_ids=[0,1])\n",
    "        else:\n",
    "            model = get_model_v2(encoder_name, decoder_name).to(device)\n",
    "\n",
    "        # Train the model and collect metrics\n",
    "        model, train_loss_history, train_acc_history, train_iou_history, val_loss_history, val_acc_history, val_iou_history = train_model(model, encoder_name, decoder_name, train_loader, val_loader, device, kwargs)\n",
    "        # save_results_and_plots(encoder_name, decoder_name, train_loss_history, train_acc_history, train_iou_history, val_loss_history, val_acc_history, val_iou_history)\n",
    "        ## show plots\n",
    "        results_df = pd.DataFrame({\n",
    "            'Epoch': list(range(1, len(train_loss_history) + 1)),\n",
    "            'Train Loss': train_loss_history,\n",
    "            'Val Loss': val_loss_history,\n",
    "            'Train Accuracy': train_acc_history,\n",
    "            'Val Accuracy': val_acc_history,\n",
    "            'Train IOU': train_iou_history,\n",
    "            'Val IOU': val_iou_history\n",
    "        })\n",
    "        plt.figure()\n",
    "        plt.plot(results_df['Epoch'], results_df['Train Loss'], label='Train Loss')\n",
    "        plt.plot(results_df['Epoch'], results_df['Val Loss'], label='Val Loss')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.legend()\n",
    "        plt.title(f'Loss Curves for {encoder_name} with {decoder_name}')\n",
    "        plt.savefig(f'loss_curves_{encoder_name}_{decoder_name}.png')\n",
    "        plt.show()\n",
    "\n",
    "        plt.figure()\n",
    "        plt.plot(results_df['Epoch'], results_df['Train Accuracy'], label='Train Accuracy')\n",
    "        plt.plot(results_df['Epoch'], results_df['Val Accuracy'], label='Val Accuracy')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Accuracy')\n",
    "        plt.legend()\n",
    "        plt.title(f'Accuracy Curves for {encoder_name} with {decoder_name}')\n",
    "        plt.savefig(f'accuracy_curves_{encoder_name}_{decoder_name}.png')\n",
    "        plt.show()\n",
    "\n",
    "        plt.figure()\n",
    "        plt.plot(results_df['Epoch'], results_df['Train IOU'], label='Train IOU')\n",
    "        plt.plot(results_df['Epoch'], results_df['Val IOU'], label='Val IOU')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('IOU')\n",
    "        plt.legend()\n",
    "        plt.title(f'IOU Curves for {encoder_name} with {decoder_name}')\n",
    "        plt.savefig(f'iou_curves_{encoder_name}_{decoder_name}.png')\n",
    "        plt.show()\n",
    "\n",
    "        # Evaluate on the test set\n",
    "        test_metrics = evaluate_model(model, test_loader, device=device)\n",
    "        for key, value in test_metrics.items():\n",
    "            if key in [\"False Discovery Rate\", \"False Negative Rate\", \"False Positive Rate\"]:\n",
    "                print(f'\"{key}\": {value:.4f},')\n",
    "            elif key == \"False Omission Rate\":\n",
    "                print(f'\"{key}\": {value:.7f},')\n",
    "            else:\n",
    "                print(f'\"{key}\": {value:.2f},')\n",
    "\n",
    "        # Save metrics to results dictionary\n",
    "        results['Encoder'].append(encoder_name)\n",
    "        results['Decoder'].append(decoder_name)\n",
    "        results['Train Loss'].append(np.min(train_loss_history))\n",
    "        results['Train Accuracy'].append(np.max(train_acc_history))\n",
    "        results['Train IoU'].append(np.max(train_iou_history))\n",
    "        results['Val Loss'].append(np.min(val_loss_history))\n",
    "        results['Val Accuracy'].append(np.max(val_acc_history))\n",
    "        results['Val IoU'].append(np.max(val_iou_history))\n",
    "        for metric_name, metric_value in test_metrics.items():\n",
    "            results[metric_name].append(metric_value)\n",
    "\n",
    "        \n",
    "        # Convert results to pandas DataFrame\n",
    "        df_new = pd.DataFrame(results)\n",
    "        \n",
    "        # Load existing DataFrame\n",
    "        try:\n",
    "            df_existing = pd.read_excel(excel_file)\n",
    "            df_combined = pd.concat([df_existing, df_new]).drop_duplicates().reset_index(drop=True)\n",
    "        except FileNotFoundError:\n",
    "            df_combined = df_new\n",
    "        \n",
    "        # Save DataFrame to Excel\n",
    "        df_combined.to_excel(excel_file, index=False)\n",
    "        \n",
    "        print(f'Results saved to {excel_file}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F35-CLscoQLg"
   },
   "source": [
    "### Run 2 best models, save and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "ANubGX6zCMrJ",
    "outputId": "64a2ea52-453e-4f6f-eaa2-992d71207229"
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Enter your best encoder-decoder combination here\n",
    "best_models = ['mit_b5--unet']\n",
    "\n",
    "kwargs = {'epochs': 10000, 'optim': {'name': 'Adam', 'params': {\n",
    "     'lr': 0.01, 'weight_decay': 3.310305423548208e-05}}, 'save': True}\n",
    "\n",
    "# Load existing results from Excel file if it exists\n",
    "excel_file = ''\n",
    "\n",
    "try:\n",
    "    existing_df = pd.read_excel(excel_file)\n",
    "    results = existing_df.to_dict(orient='list')\n",
    "    processed_models = list(existing_df.apply(lambda row: f\"{row['Encoder']}--{row['Decoder']}\", axis=1))\n",
    "except FileNotFoundError:\n",
    "    print(\"Experiment from the begining!!!\")\n",
    "    processed_models = []\n",
    "# Initialize lists to store metrics\n",
    "    results = {\n",
    "        'Encoder': [],\n",
    "        'Decoder': [],\n",
    "        'Train Loss': [],\n",
    "        'Train Accuracy': [],\n",
    "        'Train IoU': [],\n",
    "        'Val Loss': [],\n",
    "        'Val Accuracy': [],\n",
    "        'Val IoU': [],\n",
    "        'Overall Accuracy': [],\n",
    "        'Precision': [],\n",
    "        'Recall': [],\n",
    "        'F1 Score': [],\n",
    "        'Specificity': [],\n",
    "        'False Positive Rate': [],\n",
    "        'False Negative Rate': [],\n",
    "        'False Discovery Rate': [],\n",
    "        'False Omission Rate': [],\n",
    "        'Misclassification Rate': [],\n",
    "        'Matthew\\'s Correlation Coefficient': [],\n",
    "        'IoU (Jaccard Index)': [],\n",
    "        'IoU_plant' : [],\n",
    "        'Geometric Mean': []\n",
    "    }\n",
    "\n",
    "for model_info in best_models:\n",
    "    if model_info in processed_models:\n",
    "        print(f'Skipping {model_info}, already processed.')\n",
    "        continue\n",
    "    encoder_name, decoder_name = model_info.split('--')\n",
    "    print(f'Training {encoder_name} with {decoder_name}')\n",
    "    if USE_2_GPUS:\n",
    "        model = get_model(encoder_name, decoder_name)\n",
    "        model = nn.DataParallel(model).to(device)\n",
    "    else:\n",
    "        model = get_model(encoder_name, decoder_name).to(device)\n",
    "\n",
    "    # # Train the model and collect metrics\n",
    "    model, train_loss_history, train_acc_history, train_iou_history, val_loss_history, val_acc_history, val_iou_history = train_model(model, encoder_name, decoder_name, train_loader, val_loader, device, kwargs)\n",
    "\n",
    "    # # Load the best model state\n",
    "    # model.load_state_dict(torch.load(f'best_model_{encoder_name}_{decoder_name}.pth'))\n",
    "    # model.load_state_dict(torch.load(f'/content/model/{encoder_name}-{decoder_name}/model.pth'))\n",
    "    ## show plots\n",
    "    results_df = pd.DataFrame({\n",
    "        'Epoch': list(range(1, len(train_loss_history) + 1)),\n",
    "        'Train Loss': train_loss_history,\n",
    "        'Val Loss': val_loss_history,\n",
    "        'Train Accuracy': train_acc_history,\n",
    "        'Val Accuracy': val_acc_history,\n",
    "        'Train IOU': train_iou_history,\n",
    "        'Val IOU': val_iou_history\n",
    "    })\n",
    "    plt.figure()\n",
    "    plt.plot(results_df['Epoch'], results_df['Train Loss'], label='Train Loss', marker=11)\n",
    "    plt.plot(results_df['Epoch'], results_df['Val Loss'], label='Val Loss', marker=11)\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.title(f'Loss Curves for {encoder_name} with {decoder_name}')\n",
    "    plt.savefig(f'loss_curves_{encoder_name}_{decoder_name}.png')\n",
    "    plt.show()\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(results_df['Epoch'], results_df['Train Accuracy'], label='Train Accuracy', marker=11)\n",
    "    plt.plot(results_df['Epoch'], results_df['Val Accuracy'], label='Val Accuracy', marker=11)\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    plt.title(f'Accuracy Curves for {encoder_name} with {decoder_name}')\n",
    "    plt.savefig(f'accuracy_curves_{encoder_name}_{decoder_name}.png')\n",
    "    plt.show()\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(results_df['Epoch'], results_df['Train IOU'], label='Train IOU', marker=11)\n",
    "    plt.plot(results_df['Epoch'], results_df['Val IOU'], label='Val IOU', marker=11)\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('IOU')\n",
    "    plt.legend()\n",
    "    plt.title(f'IOU Curves for {encoder_name} with {decoder_name}')\n",
    "    plt.savefig(f'iou_curves_{encoder_name}_{decoder_name}.png')\n",
    "    plt.show()\n",
    "    x = model\n",
    "    # Evaluate on the test set\n",
    "    test_metrics = evaluate_model(x, test_loader, device=device)\n",
    "    for key, value in test_metrics.items():\n",
    "        if key in [\"False Discovery Rate\", \"False Negative Rate\", \"False Positive Rate\"]:\n",
    "            print(f'\"{key}\": {value:.4f}')\n",
    "        elif key == \"False Omission Rate\":\n",
    "            print(f'\"{key}\": {value:.7f}')\n",
    "        else:\n",
    "            print(f'\"{key}\": {value:.2f}')\n",
    "    print('--------------------------------------------')\n",
    "    # Append metrics to results dictionary\n",
    "    # Save metrics to results dictionary\n",
    "    results['Encoder'].append(encoder_name)\n",
    "    results['Decoder'].append(decoder_name)\n",
    "    results['Train Loss'].append(np.min(train_loss_history))\n",
    "    results['Train Accuracy'].append(np.max(train_acc_history))\n",
    "    results['Train IoU'].append(np.max(train_iou_history))\n",
    "    results['Val Loss'].append(np.min(val_loss_history))\n",
    "    results['Val Accuracy'].append(np.max(val_acc_history))\n",
    "    results['Val IoU'].append(np.max(val_iou_history))\n",
    "    for metric_name, metric_value in test_metrics.items():\n",
    "        results[metric_name].append(metric_value)\n",
    "    processed_models.append(model_info)\n",
    "\n",
    "# Convert results to pandas DataFrame\n",
    "df_new = pd.DataFrame(results)\n",
    "\n",
    "# Load existing DataFrame\n",
    "try:\n",
    "    df_existing = pd.read_excel(excel_file)\n",
    "    df_combined = pd.concat([df_existing, df_new]).drop_duplicates().reset_index(drop=True)\n",
    "except FileNotFoundError:\n",
    "    df_combined = df_new\n",
    "\n",
    "# Save DataFrame to Excel\n",
    "df_combined.to_excel(excel_file, index=False)\n",
    "\n",
    "print(f'Results saved to {excel_file}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 211
    },
    "id": "rvJAdNMeqxNR",
    "outputId": "7ea85ed1-8933-4883-8bad-df4d73ea8fb9"
   },
   "outputs": [],
   "source": [
    "next(iter(test_loader))\n",
    "images, masks = next(iter(test_loader))\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    logits = model(images.to(device).float())\n",
    "pr_masks = logits.sigmoid()\n",
    "\n",
    "for image, gt_mask, pr_mask in zip(images, masks, pr_masks):\n",
    "    plt.figure(figsize=(10, 5))\n",
    "\n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.imshow(image.numpy().transpose(1, 2, 0))  # convert CHW -> HWC\n",
    "    plt.title(\"Image\")\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "    plt.subplot(1, 3, 2)\n",
    "    plt.imshow(gt_mask.numpy().squeeze()) # just squeeze classes dim, because we have only one class\n",
    "    plt.title(\"Ground truth\")\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "    plt.subplot(1, 3, 3)\n",
    "    plt.imshow(pr_mask.cpu().numpy().squeeze()) # just squeeze classes dim, because we have only one class\n",
    "    plt.title(\"Prediction\")\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuClass": "premium",
   "machine_shape": "hm",
   "provenance": []
  },
  "gpuClass": "premium",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "cedb816d94db2648df1ed8d299ca42a0d191dc990e77464e1581386c54378e51"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
